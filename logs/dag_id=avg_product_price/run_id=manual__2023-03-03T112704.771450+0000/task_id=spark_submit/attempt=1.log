[2023-03-03 11:27:07,905] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: avg_product_price.spark_submit manual__2023-03-03T11:27:04.771450+00:00 [queued]>
[2023-03-03 11:27:07,932] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: avg_product_price.spark_submit manual__2023-03-03T11:27:04.771450+00:00 [queued]>
[2023-03-03 11:27:07,933] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-03-03 11:27:07,933] {taskinstance.py:1357} INFO - Starting attempt 1 of 2
[2023-03-03 11:27:07,933] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-03-03 11:27:07,973] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): spark_submit> on 2023-03-03 11:27:04.771450+00:00
[2023-03-03 11:27:07,979] {standard_task_runner.py:52} INFO - Started process 905 to run task
[2023-03-03 11:27:07,985] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'avg_product_price', 'spark_submit', 'manual__2023-03-03T11:27:04.771450+00:00', '--job-id', '9', '--raw', '--subdir', 'DAGS_FOLDER/avg_product_price.py', '--cfg-path', '/tmp/tmpcej80j26', '--error-file', '/tmp/tmp37af6gpj']
[2023-03-03 11:27:07,989] {standard_task_runner.py:80} INFO - Job 9: Subtask spark_submit
[2023-03-03 11:27:08,092] {task_command.py:370} INFO - Running <TaskInstance: avg_product_price.spark_submit manual__2023-03-03T11:27:04.771450+00:00 [running]> on host 7f5e973bdd66
[2023-03-03 11:27:08,260] {taskinstance.py:1571} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=ayyoub
AIRFLOW_CTX_DAG_ID=avg_product_price
AIRFLOW_CTX_TASK_ID=spark_submit
AIRFLOW_CTX_EXECUTION_DATE=2023-03-03T11:27:04.771450+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=manual__2023-03-03T11:27:04.771450+00:00
[2023-03-03 11:27:08,278] {base.py:68} INFO - Using connection ID 'spark-hadoop' for task execution.
[2023-03-03 11:27:08,280] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local[*] --name arrow-spark /hadoop-data/map_reduce/spark/average_price.py
[2023-03-03 11:27:16,699] {spark_submit.py:495} INFO - 23/03/03 11:27:16 INFO SparkContext: Running Spark version 3.3.2
[2023-03-03 11:27:16,896] {spark_submit.py:495} INFO - 23/03/03 11:27:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-03-03 11:27:17,159] {spark_submit.py:495} INFO - 23/03/03 11:27:17 INFO ResourceUtils: ==============================================================
[2023-03-03 11:27:17,161] {spark_submit.py:495} INFO - 23/03/03 11:27:17 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-03-03 11:27:17,162] {spark_submit.py:495} INFO - 23/03/03 11:27:17 INFO ResourceUtils: ==============================================================
[2023-03-03 11:27:17,163] {spark_submit.py:495} INFO - 23/03/03 11:27:17 INFO SparkContext: Submitted application: average_product_price
[2023-03-03 11:27:17,243] {spark_submit.py:495} INFO - 23/03/03 11:27:17 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-03-03 11:27:17,273] {spark_submit.py:495} INFO - 23/03/03 11:27:17 INFO ResourceProfile: Limiting resource is cpu
[2023-03-03 11:27:17,274] {spark_submit.py:495} INFO - 23/03/03 11:27:17 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-03-03 11:27:17,407] {spark_submit.py:495} INFO - 23/03/03 11:27:17 INFO SecurityManager: Changing view acls to: ***
[2023-03-03 11:27:17,409] {spark_submit.py:495} INFO - 23/03/03 11:27:17 INFO SecurityManager: Changing modify acls to: ***
[2023-03-03 11:27:17,410] {spark_submit.py:495} INFO - 23/03/03 11:27:17 INFO SecurityManager: Changing view acls groups to:
[2023-03-03 11:27:17,411] {spark_submit.py:495} INFO - 23/03/03 11:27:17 INFO SecurityManager: Changing modify acls groups to:
[2023-03-03 11:27:17,413] {spark_submit.py:495} INFO - 23/03/03 11:27:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-03-03 11:27:18,086] {spark_submit.py:495} INFO - 23/03/03 11:27:18 INFO Utils: Successfully started service 'sparkDriver' on port 44831.
[2023-03-03 11:27:18,186] {spark_submit.py:495} INFO - 23/03/03 11:27:18 INFO SparkEnv: Registering MapOutputTracker
[2023-03-03 11:27:18,292] {spark_submit.py:495} INFO - 23/03/03 11:27:18 INFO SparkEnv: Registering BlockManagerMaster
[2023-03-03 11:27:18,328] {spark_submit.py:495} INFO - 23/03/03 11:27:18 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-03-03 11:27:18,330] {spark_submit.py:495} INFO - 23/03/03 11:27:18 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-03-03 11:27:18,338] {spark_submit.py:495} INFO - 23/03/03 11:27:18 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-03-03 11:27:18,413] {spark_submit.py:495} INFO - 23/03/03 11:27:18 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-852e7bb3-1835-42d5-a58f-5099e7aa95a5
[2023-03-03 11:27:18,466] {spark_submit.py:495} INFO - 23/03/03 11:27:18 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-03-03 11:27:18,515] {spark_submit.py:495} INFO - 23/03/03 11:27:18 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-03-03 11:27:19,082] {spark_submit.py:495} INFO - 23/03/03 11:27:19 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-03-03 11:27:19,418] {spark_submit.py:495} INFO - 23/03/03 11:27:19 INFO Executor: Starting executor ID driver on host 7f5e973bdd66
[2023-03-03 11:27:19,442] {spark_submit.py:495} INFO - 23/03/03 11:27:19 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-03-03 11:27:19,508] {spark_submit.py:495} INFO - 23/03/03 11:27:19 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35627.
[2023-03-03 11:27:19,508] {spark_submit.py:495} INFO - 23/03/03 11:27:19 INFO NettyBlockTransferService: Server created on 7f5e973bdd66:35627
[2023-03-03 11:27:19,512] {spark_submit.py:495} INFO - 23/03/03 11:27:19 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-03-03 11:27:19,532] {spark_submit.py:495} INFO - 23/03/03 11:27:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None)
[2023-03-03 11:27:19,542] {spark_submit.py:495} INFO - 23/03/03 11:27:19 INFO BlockManagerMasterEndpoint: Registering block manager 7f5e973bdd66:35627 with 434.4 MiB RAM, BlockManagerId(driver, 7f5e973bdd66, 35627, None)
[2023-03-03 11:27:19,561] {spark_submit.py:495} INFO - 23/03/03 11:27:19 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None)
[2023-03-03 11:27:19,564] {spark_submit.py:495} INFO - 23/03/03 11:27:19 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7f5e973bdd66, 35627, None)
[2023-03-03 11:27:20,311] {spark_submit.py:495} INFO - /opt/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/sql/context.py:114: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.
[2023-03-03 11:27:20,781] {spark_submit.py:495} INFO - 23/03/03 11:27:20 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-03-03 11:27:20,832] {spark_submit.py:495} INFO - 23/03/03 11:27:20 INFO SharedState: Warehouse path is 'file:/home/***/spark-warehouse'.
[2023-03-03 11:28:59,532] {spark_submit.py:495} INFO - 23/03/03 11:28:52 INFO InMemoryFileIndex: It took 74623 ms to list leaf files for 1 paths.
[2023-03-03 11:28:03,599] {base_job.py:229} ERROR - LocalTaskJob heartbeat got an exception
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 3141, in _wrap_pool_connect
    return fn()
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 301, in connect
    return _ConnectionFairy._checkout(self)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 755, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 419, in checkout
    rec = pool._do_get()
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/impl.py", line 259, in _do_get
    return self._create_connection()
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 247, in _create_connection
    return _ConnectionRecord(self)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 362, in __init__
    self.__connect(first_connect_check=True)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 605, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 72, in __exit__
    with_traceback=exc_tb,
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 599, in __connect
    connection = pool._invoke_creator(self)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 583, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/usr/local/lib/python3.7/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/airflow/jobs/base_job.py", line 201, in heartbeat
    session.merge(self)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2877, in merge
    _resolve_conflict_map=_resolve_conflict_map,
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2950, in _merge
    merged = self.get(mapper.class_, key[1], identity_token=key[2])
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2702, in get
    identity_token=identity_token,
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2800, in _get_impl
    load_options=load_options,
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/loading.py", line 535, in load_on_pk_identity
    bind_arguments=bind_arguments,
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1669, in execute
    conn = self._connection_for_bind(bind, close_with_result=True)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1520, in _connection_for_bind
    engine, execution_options
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 3095, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 91, in __init__
    else engine.raw_connection()
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 3174, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 3145, in _wrap_pool_connect
    e, dialect, self
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 2004, in _handle_dbapi_exception_noconnection
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 3141, in _wrap_pool_connect
    return fn()
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 301, in connect
    return _ConnectionFairy._checkout(self)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 755, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 419, in checkout
    rec = pool._do_get()
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/impl.py", line 259, in _do_get
    return self._create_connection()
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 247, in _create_connection
    return _ConnectionRecord(self)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 362, in __init__
    self.__connect(first_connect_check=True)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 605, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 72, in __exit__
    with_traceback=exc_tb,
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 599, in __connect
    connection = pool._invoke_creator(self)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 583, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/usr/local/lib/python3.7/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: http://sqlalche.me/e/14/e3q8)
[2023-03-03 11:30:18,995] {spark_submit.py:495} INFO - 23/03/03 11:30:18 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 129258 ms exceeds timeout 120000 ms
[2023-03-03 11:30:19,086] {spark_submit.py:495} INFO - 23/03/03 11:30:19 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@45334db3)) by listener HeartbeatReceiver took 1.1096702s.
[2023-03-03 11:30:19,089] {spark_submit.py:495} INFO - 23/03/03 11:30:19 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@45334db3)) by listener AppStatusListener took 25.5054546s.
[2023-03-03 11:30:19,093] {spark_submit.py:495} INFO - 23/03/03 11:30:19 WARN SparkContext: Killing executors is not supported by current scheduler.
[2023-03-03 11:30:19,101] {spark_submit.py:495} INFO - 23/03/03 11:30:19 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@45334db3)) by listener ExecutionListenerBus took 1.3541067s.
[2023-03-03 11:30:19,185] {spark_submit.py:495} INFO - 23/03/03 11:30:19 INFO Executor: Told to re-register on heartbeat
[2023-03-03 11:30:19,186] {spark_submit.py:495} INFO - 23/03/03 11:30:19 INFO BlockManager: BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None) re-registering with master
[2023-03-03 11:30:19,190] {spark_submit.py:495} INFO - 23/03/03 11:30:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None)
[2023-03-03 11:30:19,233] {spark_submit.py:495} INFO - 23/03/03 11:30:19 ERROR Inbox: Ignoring error
[2023-03-03 11:30:19,235] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:19,238] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:19,242] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:19,243] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-03 11:30:19,243] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-03 11:30:19,244] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-03 11:30:19,245] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-03 11:30:19,248] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-03 11:30:19,251] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-03 11:30:19,252] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-03 11:30:19,257] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-03 11:30:19,258] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-03 11:30:19,261] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:19,262] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:19,264] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:19,265] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:19,265] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:19,266] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:19,269] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-03 11:30:19,271] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-03 11:30:19,277] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-03 11:30:19,279] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-03 11:30:19,287] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@7f5e973bdd66:44831
[2023-03-03 11:30:19,290] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-03 11:30:19,294] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-03 11:30:19,294] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-03 11:30:19,312] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-03 11:30:19,319] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:19,320] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:19,320] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:19,321] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:19,326] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2023-03-03 11:30:19,327] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2023-03-03 11:30:19,328] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2023-03-03 11:30:19,328] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2023-03-03 11:30:19,329] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2023-03-03 11:30:19,340] {spark_submit.py:495} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2023-03-03 11:30:19,343] {spark_submit.py:495} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2023-03-03 11:30:19,344] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2023-03-03 11:30:19,349] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2023-03-03 11:30:19,359] {spark_submit.py:495} INFO - ... 19 more
[2023-03-03 11:30:19,361] {spark_submit.py:495} INFO - 23/03/03 11:30:19 WARN Executor: Issue communicating with driver in heartbeater
[2023-03-03 11:30:19,362] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:19,364] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:19,367] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:19,374] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-03-03 11:30:19,374] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-03-03 11:30:19,376] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2023-03-03 11:30:19,376] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-03-03 11:30:19,377] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-03-03 11:30:19,378] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-03-03 11:30:19,379] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:19,381] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-03-03 11:30:19,382] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-03-03 11:30:19,383] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:19,384] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-03-03 11:30:19,385] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-03-03 11:30:19,387] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-03 11:30:19,387] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-03 11:30:19,388] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-03 11:30:19,389] {spark_submit.py:495} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:19,391] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:19,392] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:19,393] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-03 11:30:19,394] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-03 11:30:19,394] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-03 11:30:19,395] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-03 11:30:19,396] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-03 11:30:19,397] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-03 11:30:19,398] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-03 11:30:19,400] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-03 11:30:19,401] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-03 11:30:19,402] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:19,404] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:19,407] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:19,410] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:19,416] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:19,424] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:19,425] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-03 11:30:19,426] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:19,427] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@7f5e973bdd66:44831
[2023-03-03 11:30:19,427] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-03 11:30:19,428] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-03 11:30:19,429] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-03 11:30:19,430] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-03 11:30:19,431] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:19,432] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:19,434] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:19,437] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:19,438] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2023-03-03 11:30:19,439] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2023-03-03 11:30:19,441] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2023-03-03 11:30:19,442] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2023-03-03 11:30:19,443] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2023-03-03 11:30:19,444] {spark_submit.py:495} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2023-03-03 11:30:19,445] {spark_submit.py:495} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2023-03-03 11:30:19,447] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2023-03-03 11:30:19,448] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2023-03-03 11:30:19,449] {spark_submit.py:495} INFO - ... 19 more
[2023-03-03 11:30:19,450] {spark_submit.py:495} INFO - 23/03/03 11:30:19 INFO Executor: Told to re-register on heartbeat
[2023-03-03 11:30:19,451] {spark_submit.py:495} INFO - 23/03/03 11:30:19 INFO BlockManager: BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None) re-registering with master
[2023-03-03 11:30:19,455] {spark_submit.py:495} INFO - 23/03/03 11:30:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None)
[2023-03-03 11:30:19,457] {spark_submit.py:495} INFO - 23/03/03 11:30:19 ERROR Inbox: Ignoring error
[2023-03-03 11:30:19,459] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:19,463] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:19,464] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:19,464] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-03 11:30:19,513] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-03 11:30:19,514] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-03 11:30:19,515] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-03 11:30:19,516] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-03 11:30:19,516] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-03 11:30:19,517] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-03 11:30:19,520] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-03 11:30:19,527] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-03 11:30:19,529] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:19,530] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:19,532] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:19,533] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:19,543] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:19,545] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:19,548] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-03 11:30:19,550] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-03 11:30:19,551] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-03 11:30:19,552] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-03 11:30:19,554] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@7f5e973bdd66:44831
[2023-03-03 11:30:19,555] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-03 11:30:19,555] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-03 11:30:19,556] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-03 11:30:19,556] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-03 11:30:19,557] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:19,560] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:19,561] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:19,561] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:19,562] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2023-03-03 11:30:19,562] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2023-03-03 11:30:19,563] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2023-03-03 11:30:19,567] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2023-03-03 11:30:19,568] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2023-03-03 11:30:19,570] {spark_submit.py:495} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2023-03-03 11:30:19,572] {spark_submit.py:495} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2023-03-03 11:30:19,573] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2023-03-03 11:30:19,573] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2023-03-03 11:30:19,578] {spark_submit.py:495} INFO - ... 19 more
[2023-03-03 11:30:19,579] {spark_submit.py:495} INFO - 23/03/03 11:30:19 WARN Executor: Issue communicating with driver in heartbeater
[2023-03-03 11:30:19,579] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:19,580] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:19,581] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:19,582] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-03-03 11:30:19,583] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-03-03 11:30:19,583] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2023-03-03 11:30:19,584] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-03-03 11:30:19,585] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-03-03 11:30:19,586] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-03-03 11:30:19,587] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:19,588] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-03-03 11:30:19,589] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-03-03 11:30:19,591] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:19,592] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-03-03 11:30:19,593] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-03-03 11:30:19,594] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-03 11:30:19,596] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-03 11:30:19,597] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-03 11:30:19,598] {spark_submit.py:495} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:19,599] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:19,600] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:19,601] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-03 11:30:19,602] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-03 11:30:19,603] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-03 11:30:19,604] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-03 11:30:19,605] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-03 11:30:19,607] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-03 11:30:19,607] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-03 11:30:19,608] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-03 11:30:19,609] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-03 11:30:19,610] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:19,611] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:19,611] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:19,612] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:19,613] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:19,613] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:19,614] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-03 11:30:19,616] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:19,617] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@7f5e973bdd66:44831
[2023-03-03 11:30:19,618] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-03 11:30:19,619] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-03 11:30:19,620] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-03 11:30:19,623] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-03 11:30:19,623] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:19,624] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:19,625] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:19,626] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:19,626] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2023-03-03 11:30:19,627] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2023-03-03 11:30:19,628] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2023-03-03 11:30:19,629] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2023-03-03 11:30:19,629] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2023-03-03 11:30:19,632] {spark_submit.py:495} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2023-03-03 11:30:19,634] {spark_submit.py:495} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2023-03-03 11:30:19,637] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2023-03-03 11:30:19,641] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2023-03-03 11:30:19,641] {spark_submit.py:495} INFO - ... 19 more
[2023-03-03 11:30:19,642] {spark_submit.py:495} INFO - 23/03/03 11:30:19 INFO Executor: Told to re-register on heartbeat
[2023-03-03 11:30:19,643] {spark_submit.py:495} INFO - 23/03/03 11:30:19 INFO BlockManager: BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None) re-registering with master
[2023-03-03 11:30:19,644] {spark_submit.py:495} INFO - 23/03/03 11:30:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None)
[2023-03-03 11:30:19,645] {spark_submit.py:495} INFO - 23/03/03 11:30:19 ERROR Inbox: Ignoring error
[2023-03-03 11:30:19,647] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:19,647] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:19,648] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:19,649] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-03 11:30:19,650] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-03 11:30:19,650] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-03 11:30:19,651] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-03 11:30:19,652] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-03 11:30:19,652] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-03 11:30:19,653] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-03 11:30:19,654] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-03 11:30:19,655] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-03 11:30:19,657] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:19,658] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:19,658] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:19,659] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:19,659] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:19,660] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:19,661] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-03 11:30:19,661] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-03 11:30:19,662] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-03 11:30:19,663] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-03 11:30:19,663] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@7f5e973bdd66:44831
[2023-03-03 11:30:19,664] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-03 11:30:19,664] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-03 11:30:19,665] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-03 11:30:19,666] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-03 11:30:19,667] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:19,667] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:19,668] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:19,669] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:19,669] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:19,671] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:19,671] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:19,672] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:19,674] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:19,679] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:19,679] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:19,680] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:19,681] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:19,682] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:19,682] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:19,683] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:19,684] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:19,685] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:19,686] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:19,688] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:19,689] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:19,690] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:19,690] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:19,691] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-03 11:30:19,692] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-03 11:30:19,693] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:19,693] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-03 11:30:19,694] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-03 11:30:19,695] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-03 11:30:19,696] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-03 11:30:19,697] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-03 11:30:19,698] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-03 11:30:19,698] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:19,699] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:19,700] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:19,700] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:19,708] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-03 11:30:19,710] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-03 11:30:19,712] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-03 11:30:19,713] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-03 11:30:19,714] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-03 11:30:19,715] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-03 11:30:19,716] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:19,717] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:19,719] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:19,720] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:19,720] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:19,722] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:19,723] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:19,725] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:19,725] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:19,726] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:19,727] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-03 11:30:19,727] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-03 11:30:19,728] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-03 11:30:19,763] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-03 11:30:19,764] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-03 11:30:19,764] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-03 11:30:19,766] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:19,767] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:19,768] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:19,769] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:19,770] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:19,771] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:19,772] {spark_submit.py:495} INFO - 23/03/03 11:30:19 WARN Executor: Issue communicating with driver in heartbeater
[2023-03-03 11:30:19,772] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:19,773] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:19,774] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:19,774] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-03-03 11:30:19,775] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-03-03 11:30:19,776] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2023-03-03 11:30:19,776] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-03-03 11:30:19,777] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-03-03 11:30:19,777] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-03-03 11:30:19,779] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:19,780] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-03-03 11:30:19,781] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-03-03 11:30:19,782] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:19,783] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-03-03 11:30:19,784] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-03-03 11:30:19,786] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-03 11:30:19,787] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-03 11:30:19,788] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-03 11:30:19,788] {spark_submit.py:495} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:19,789] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:19,790] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:19,791] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-03 11:30:19,791] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-03 11:30:19,792] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-03 11:30:19,793] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-03 11:30:19,793] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-03 11:30:19,794] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-03 11:30:19,794] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-03 11:30:19,796] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-03 11:30:19,797] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-03 11:30:19,798] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:19,799] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:19,799] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:19,800] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:19,801] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:19,802] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:19,803] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-03 11:30:19,804] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:19,805] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@7f5e973bdd66:44831
[2023-03-03 11:30:19,809] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-03 11:30:19,810] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-03 11:30:19,811] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-03 11:30:19,812] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-03 11:30:19,812] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:19,813] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:19,814] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:19,816] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:19,817] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:19,818] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:19,819] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:19,820] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:19,822] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:19,826] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:19,829] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:19,829] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:19,830] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:19,831] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:19,831] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:19,832] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:19,833] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:19,834] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:19,834] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:19,835] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:19,835] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:19,836] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:19,836] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:19,841] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-03 11:30:19,854] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-03 11:30:19,855] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:19,856] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-03 11:30:19,856] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-03 11:30:19,857] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-03 11:30:19,858] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-03 11:30:19,859] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-03 11:30:19,861] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-03 11:30:19,863] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:19,864] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:19,865] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:19,866] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:19,867] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-03 11:30:19,867] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-03 11:30:19,868] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-03 11:30:19,868] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-03 11:30:19,869] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-03 11:30:19,870] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-03 11:30:19,871] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:19,872] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:19,872] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:19,873] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:19,874] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:19,882] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:19,884] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:19,885] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:19,886] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:19,887] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:19,887] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-03 11:30:19,888] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-03 11:30:19,889] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-03 11:30:19,889] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-03 11:30:19,890] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-03 11:30:19,890] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-03 11:30:19,891] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:19,892] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:19,893] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:19,894] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:19,895] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:19,896] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:19,897] {spark_submit.py:495} INFO - 23/03/03 11:30:19 INFO Executor: Told to re-register on heartbeat
[2023-03-03 11:30:19,898] {spark_submit.py:495} INFO - 23/03/03 11:30:19 INFO BlockManager: BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None) re-registering with master
[2023-03-03 11:30:19,899] {spark_submit.py:495} INFO - 23/03/03 11:30:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None)
[2023-03-03 11:30:19,900] {spark_submit.py:495} INFO - 23/03/03 11:30:19 WARN Executor: Issue communicating with driver in heartbeater
[2023-03-03 11:30:19,901] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:19,901] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:19,903] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:19,904] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-03-03 11:30:19,904] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-03-03 11:30:19,905] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2023-03-03 11:30:19,905] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-03-03 11:30:19,906] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-03-03 11:30:19,906] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-03-03 11:30:19,908] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:19,909] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-03-03 11:30:19,910] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-03-03 11:30:19,911] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:19,912] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-03-03 11:30:19,914] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-03-03 11:30:19,915] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-03 11:30:19,916] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-03 11:30:19,917] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-03 11:30:19,917] {spark_submit.py:495} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:19,918] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:19,919] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:19,919] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-03 11:30:19,920] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-03 11:30:19,920] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-03 11:30:19,921] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-03 11:30:19,922] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-03 11:30:19,922] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-03 11:30:19,923] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-03 11:30:19,924] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-03 11:30:19,924] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-03 11:30:19,925] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:19,926] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:19,927] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:19,927] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:19,928] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:19,929] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:19,930] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-03 11:30:19,930] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:19,931] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@7f5e973bdd66:44831
[2023-03-03 11:30:19,932] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-03 11:30:19,932] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-03 11:30:19,933] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-03 11:30:19,935] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-03 11:30:19,936] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:19,937] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:19,938] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:19,939] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:19,940] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2023-03-03 11:30:19,940] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2023-03-03 11:30:19,941] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2023-03-03 11:30:19,942] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2023-03-03 11:30:19,942] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2023-03-03 11:30:19,943] {spark_submit.py:495} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2023-03-03 11:30:19,944] {spark_submit.py:495} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2023-03-03 11:30:19,944] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2023-03-03 11:30:19,945] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2023-03-03 11:30:19,946] {spark_submit.py:495} INFO - ... 19 more
[2023-03-03 11:30:19,946] {spark_submit.py:495} INFO - 23/03/03 11:30:19 ERROR Inbox: Ignoring error
[2023-03-03 11:30:19,947] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:19,947] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:19,948] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:19,949] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-03 11:30:19,949] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-03 11:30:19,951] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-03 11:30:19,953] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-03 11:30:19,954] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-03 11:30:19,955] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-03 11:30:19,958] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-03 11:30:19,960] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-03 11:30:19,960] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-03 11:30:19,961] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:19,961] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:19,962] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:19,963] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:19,964] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:19,965] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:19,966] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-03 11:30:19,967] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-03 11:30:19,967] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-03 11:30:19,969] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-03 11:30:19,970] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@7f5e973bdd66:44831
[2023-03-03 11:30:19,972] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-03 11:30:19,974] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-03 11:30:19,978] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-03 11:30:19,979] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-03 11:30:19,979] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:19,980] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:19,981] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:19,981] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:19,982] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2023-03-03 11:30:19,983] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2023-03-03 11:30:19,984] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2023-03-03 11:30:19,984] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2023-03-03 11:30:19,985] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2023-03-03 11:30:19,986] {spark_submit.py:495} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2023-03-03 11:30:19,986] {spark_submit.py:495} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2023-03-03 11:30:19,987] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2023-03-03 11:30:19,990] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2023-03-03 11:30:19,993] {spark_submit.py:495} INFO - ... 19 more
[2023-03-03 11:30:19,994] {spark_submit.py:495} INFO - 23/03/03 11:30:19 INFO Executor: Told to re-register on heartbeat
[2023-03-03 11:30:19,995] {spark_submit.py:495} INFO - 23/03/03 11:30:19 INFO BlockManager: BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None) re-registering with master
[2023-03-03 11:30:19,996] {spark_submit.py:495} INFO - 23/03/03 11:30:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None)
[2023-03-03 11:30:19,997] {spark_submit.py:495} INFO - 23/03/03 11:30:19 ERROR Inbox: Ignoring error
[2023-03-03 11:30:19,997] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:19,998] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:19,999] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:19,999] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-03 11:30:20,000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-03 11:30:20,001] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-03 11:30:20,002] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-03 11:30:20,003] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-03 11:30:20,003] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-03 11:30:20,004] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-03 11:30:20,005] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-03 11:30:20,007] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-03 11:30:20,009] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:20,010] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:20,011] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:20,012] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:20,014] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:20,016] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:20,017] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-03 11:30:20,019] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-03 11:30:20,020] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-03 11:30:20,020] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-03 11:30:20,021] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@7f5e973bdd66:44831
[2023-03-03 11:30:20,022] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-03 11:30:20,024] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-03 11:30:20,025] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-03 11:30:20,032] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-03 11:30:20,033] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,035] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:20,036] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:20,038] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,039] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2023-03-03 11:30:20,041] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2023-03-03 11:30:20,042] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2023-03-03 11:30:20,043] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2023-03-03 11:30:20,043] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2023-03-03 11:30:20,044] {spark_submit.py:495} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2023-03-03 11:30:20,045] {spark_submit.py:495} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2023-03-03 11:30:20,045] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2023-03-03 11:30:20,046] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2023-03-03 11:30:20,046] {spark_submit.py:495} INFO - ... 19 more
[2023-03-03 11:30:20,047] {spark_submit.py:495} INFO - 23/03/03 11:30:19 WARN Executor: Issue communicating with driver in heartbeater
[2023-03-03 11:30:20,047] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:20,048] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:20,049] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:20,049] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-03-03 11:30:20,050] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-03-03 11:30:20,051] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2023-03-03 11:30:20,051] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-03-03 11:30:20,052] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-03-03 11:30:20,053] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-03-03 11:30:20,054] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:20,055] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-03-03 11:30:20,056] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-03-03 11:30:20,057] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:20,061] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-03-03 11:30:20,062] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-03-03 11:30:20,063] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-03 11:30:20,064] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-03 11:30:20,064] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-03 11:30:20,065] {spark_submit.py:495} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:20,066] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:20,067] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:20,068] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-03 11:30:20,069] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-03 11:30:20,070] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-03 11:30:20,071] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-03 11:30:20,071] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-03 11:30:20,072] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-03 11:30:20,073] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-03 11:30:20,073] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-03 11:30:20,074] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-03 11:30:20,075] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:20,075] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:20,076] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:20,077] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:20,077] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:20,078] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:20,079] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-03 11:30:20,080] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:20,080] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@7f5e973bdd66:44831
[2023-03-03 11:30:20,081] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-03 11:30:20,082] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-03 11:30:20,083] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-03 11:30:20,083] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-03 11:30:20,084] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,085] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:20,085] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:20,086] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,087] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2023-03-03 11:30:20,087] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2023-03-03 11:30:20,088] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2023-03-03 11:30:20,089] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2023-03-03 11:30:20,089] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2023-03-03 11:30:20,090] {spark_submit.py:495} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2023-03-03 11:30:20,090] {spark_submit.py:495} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2023-03-03 11:30:20,091] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2023-03-03 11:30:20,091] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2023-03-03 11:30:20,092] {spark_submit.py:495} INFO - ... 19 more
[2023-03-03 11:30:20,093] {spark_submit.py:495} INFO - 23/03/03 11:30:19 INFO Executor: Told to re-register on heartbeat
[2023-03-03 11:30:20,094] {spark_submit.py:495} INFO - 23/03/03 11:30:19 INFO BlockManager: BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None) re-registering with master
[2023-03-03 11:30:20,094] {spark_submit.py:495} INFO - 23/03/03 11:30:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None)
[2023-03-03 11:30:20,095] {spark_submit.py:495} INFO - 23/03/03 11:30:19 ERROR Inbox: Ignoring error
[2023-03-03 11:30:20,095] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:20,096] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:20,097] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:20,098] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-03 11:30:20,098] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-03 11:30:20,099] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-03 11:30:20,099] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-03 11:30:20,100] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-03 11:30:20,100] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-03 11:30:20,101] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-03 11:30:20,102] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-03 11:30:20,102] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-03 11:30:20,103] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:20,104] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:20,105] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:20,106] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:20,106] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:20,107] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:20,107] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-03 11:30:20,108] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-03 11:30:20,109] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-03 11:30:20,110] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-03 11:30:20,110] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@7f5e973bdd66:44831
[2023-03-03 11:30:20,111] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-03 11:30:20,111] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-03 11:30:20,112] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-03 11:30:20,113] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-03 11:30:20,113] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,114] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:20,114] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:20,115] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,116] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:20,116] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:20,117] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:20,117] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:20,118] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:20,119] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:20,119] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:20,120] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,121] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:20,122] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:20,122] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,123] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:20,125] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:20,126] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:20,127] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:20,127] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:20,128] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:20,128] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:20,129] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,129] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-03 11:30:20,130] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-03 11:30:20,131] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:20,131] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-03 11:30:20,132] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-03 11:30:20,133] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-03 11:30:20,133] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-03 11:30:20,134] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-03 11:30:20,134] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-03 11:30:20,135] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,135] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:20,136] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:20,137] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:20,137] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-03 11:30:20,138] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-03 11:30:20,138] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-03 11:30:20,139] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-03 11:30:20,140] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-03 11:30:20,140] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-03 11:30:20,141] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,141] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:20,142] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:20,143] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,143] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:20,144] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:20,144] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:20,145] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:20,146] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:20,146] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:20,147] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-03 11:30:20,147] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-03 11:30:20,148] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-03 11:30:20,148] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-03 11:30:20,149] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-03 11:30:20,149] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-03 11:30:20,150] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:20,150] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:20,151] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:20,152] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:20,152] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:20,153] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:20,154] {spark_submit.py:495} INFO - 23/03/03 11:30:19 WARN Executor: Issue communicating with driver in heartbeater
[2023-03-03 11:30:20,155] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:20,155] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:20,156] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:20,156] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-03-03 11:30:20,157] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-03-03 11:30:20,157] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2023-03-03 11:30:20,158] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-03-03 11:30:20,158] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-03-03 11:30:20,159] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-03-03 11:30:20,159] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:20,160] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-03-03 11:30:20,160] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-03-03 11:30:20,161] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:20,162] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-03-03 11:30:20,162] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-03-03 11:30:20,163] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-03 11:30:20,163] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-03 11:30:20,164] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-03 11:30:20,164] {spark_submit.py:495} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:20,165] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:20,165] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:20,166] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-03 11:30:20,166] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-03 11:30:20,167] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-03 11:30:20,167] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-03 11:30:20,168] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-03 11:30:20,168] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-03 11:30:20,169] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-03 11:30:20,170] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-03 11:30:20,171] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-03 11:30:20,172] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:20,172] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:20,173] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:20,173] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:20,174] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:20,175] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:20,175] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-03 11:30:20,176] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:20,177] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@7f5e973bdd66:44831
[2023-03-03 11:30:20,177] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-03 11:30:20,178] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-03 11:30:20,178] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-03 11:30:20,179] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-03 11:30:20,179] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,180] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:20,180] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:20,181] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,182] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:20,182] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:20,183] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:20,183] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:20,184] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:20,185] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:20,185] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:20,186] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,186] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:20,187] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:20,187] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,188] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:20,188] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:20,189] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:20,190] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:20,190] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:20,191] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:20,192] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:20,192] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,193] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-03 11:30:20,193] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-03 11:30:20,194] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:20,194] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-03 11:30:20,195] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-03 11:30:20,195] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-03 11:30:20,196] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-03 11:30:20,196] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-03 11:30:20,197] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-03 11:30:20,198] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,198] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:20,199] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:20,199] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:20,200] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-03 11:30:20,200] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-03 11:30:20,201] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-03 11:30:20,201] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-03 11:30:20,202] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-03 11:30:20,202] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-03 11:30:20,203] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,203] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:20,204] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:20,204] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,205] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:20,205] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:20,206] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:20,206] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:20,207] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:20,207] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:20,208] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-03 11:30:20,209] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-03 11:30:20,209] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-03 11:30:20,210] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-03 11:30:20,210] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-03 11:30:20,211] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-03 11:30:20,211] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:20,212] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:20,213] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:20,213] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:20,214] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:20,214] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:20,215] {spark_submit.py:495} INFO - 23/03/03 11:30:19 INFO Executor: Told to re-register on heartbeat
[2023-03-03 11:30:20,215] {spark_submit.py:495} INFO - 23/03/03 11:30:19 INFO BlockManager: BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None) re-registering with master
[2023-03-03 11:30:20,216] {spark_submit.py:495} INFO - 23/03/03 11:30:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None)
[2023-03-03 11:30:20,217] {spark_submit.py:495} INFO - 23/03/03 11:30:19 ERROR Inbox: Ignoring error
[2023-03-03 11:30:20,217] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:20,218] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:20,218] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:20,219] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-03 11:30:20,219] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-03 11:30:20,220] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-03 11:30:20,220] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-03 11:30:20,221] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-03 11:30:20,222] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-03 11:30:20,222] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-03 11:30:20,223] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-03 11:30:20,223] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-03 11:30:20,224] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:20,224] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:20,225] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:20,225] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:20,226] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:20,226] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:20,227] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-03 11:30:20,228] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-03 11:30:20,228] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-03 11:30:20,229] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-03 11:30:20,230] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@7f5e973bdd66:44831
[2023-03-03 11:30:20,230] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-03 11:30:20,231] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-03 11:30:20,231] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-03 11:30:20,232] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-03 11:30:20,233] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,233] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:20,234] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:20,234] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,235] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:20,236] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:20,236] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:20,237] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:20,237] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:20,238] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:20,238] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:20,239] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,240] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:20,240] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:20,241] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,241] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:20,242] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:20,242] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:20,243] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:20,243] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:20,244] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:20,245] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:20,245] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,246] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-03 11:30:20,246] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-03 11:30:20,247] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:20,247] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-03 11:30:20,248] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-03 11:30:20,249] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-03 11:30:20,249] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-03 11:30:20,250] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-03 11:30:20,250] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-03 11:30:20,251] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,251] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:20,252] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:20,253] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:20,253] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-03 11:30:20,254] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-03 11:30:20,255] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-03 11:30:20,256] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-03 11:30:20,257] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-03 11:30:20,258] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-03 11:30:20,258] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,259] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:20,260] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:20,260] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,261] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:20,262] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:20,262] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:20,263] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:20,264] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:20,264] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:20,265] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-03 11:30:20,265] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-03 11:30:20,266] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-03 11:30:20,266] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-03 11:30:20,267] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-03 11:30:20,267] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-03 11:30:20,268] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:20,268] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:20,269] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:20,269] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:20,270] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:20,270] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:20,271] {spark_submit.py:495} INFO - 23/03/03 11:30:19 WARN Executor: Issue communicating with driver in heartbeater
[2023-03-03 11:30:20,272] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:20,272] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:20,273] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:20,275] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-03-03 11:30:20,279] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-03-03 11:30:20,281] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2023-03-03 11:30:20,282] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-03-03 11:30:20,283] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-03-03 11:30:20,283] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-03-03 11:30:20,284] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:20,284] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-03-03 11:30:20,286] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-03-03 11:30:20,286] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:20,287] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-03-03 11:30:20,288] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-03-03 11:30:20,288] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-03 11:30:20,289] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-03 11:30:20,289] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-03 11:30:20,290] {spark_submit.py:495} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:20,290] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:20,291] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:20,292] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-03 11:30:20,292] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-03 11:30:20,293] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-03 11:30:20,307] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-03 11:30:20,308] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-03 11:30:20,309] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-03 11:30:20,309] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-03 11:30:20,310] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-03 11:30:20,310] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-03 11:30:20,311] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:20,312] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:20,313] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:20,314] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:20,315] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:20,316] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:20,317] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-03 11:30:20,320] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:20,322] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@7f5e973bdd66:44831
[2023-03-03 11:30:20,323] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-03 11:30:20,326] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-03 11:30:20,327] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-03 11:30:20,327] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-03 11:30:20,328] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,332] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:20,333] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:20,335] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,336] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:20,337] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:20,342] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:20,343] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:20,344] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:20,344] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:20,352] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:20,354] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,356] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:20,358] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:20,360] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,361] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:20,363] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:20,363] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:20,364] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:20,366] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:20,368] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:20,370] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:20,371] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,374] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-03 11:30:20,380] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-03 11:30:20,381] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:20,381] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-03 11:30:20,382] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-03 11:30:20,382] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-03 11:30:20,383] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-03 11:30:20,383] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-03 11:30:20,384] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-03 11:30:20,384] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,386] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:20,386] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:20,387] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:20,388] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-03 11:30:20,388] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-03 11:30:20,389] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-03 11:30:20,390] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-03 11:30:20,391] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-03 11:30:20,392] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-03 11:30:20,393] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,393] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:20,395] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:20,399] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,400] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:20,401] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:20,401] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:20,402] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:20,402] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:20,403] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:20,403] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-03 11:30:20,404] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-03 11:30:20,405] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-03 11:30:20,405] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-03 11:30:20,406] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-03 11:30:20,410] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-03 11:30:20,410] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:20,412] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:20,414] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:20,415] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:20,416] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:20,416] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:20,417] {spark_submit.py:495} INFO - 23/03/03 11:30:19 INFO Executor: Told to re-register on heartbeat
[2023-03-03 11:30:20,418] {spark_submit.py:495} INFO - 23/03/03 11:30:19 INFO BlockManager: BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None) re-registering with master
[2023-03-03 11:30:20,418] {spark_submit.py:495} INFO - 23/03/03 11:30:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None)
[2023-03-03 11:30:20,420] {spark_submit.py:495} INFO - 23/03/03 11:30:19 ERROR Inbox: Ignoring error
[2023-03-03 11:30:20,421] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:20,421] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:20,423] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:20,425] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-03 11:30:20,426] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-03 11:30:20,427] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-03 11:30:20,427] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-03 11:30:20,433] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-03 11:30:20,434] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-03 11:30:20,435] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-03 11:30:20,436] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-03 11:30:20,437] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-03 11:30:20,438] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:20,439] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:20,439] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:20,440] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:20,440] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:20,441] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:20,441] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-03 11:30:20,442] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-03 11:30:20,442] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-03 11:30:20,443] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-03 11:30:20,444] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@7f5e973bdd66:44831
[2023-03-03 11:30:20,444] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-03 11:30:20,445] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-03 11:30:20,446] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-03 11:30:20,446] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-03 11:30:20,447] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,447] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:20,448] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:20,449] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,449] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:20,450] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:20,450] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:20,451] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:20,452] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:20,453] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:20,455] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:20,456] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,457] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:20,458] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:20,459] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,460] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:20,461] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:20,461] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:20,462] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:20,462] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:20,463] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:20,464] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:20,464] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,465] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-03 11:30:20,466] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-03 11:30:20,466] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:20,467] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-03 11:30:20,468] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-03 11:30:20,469] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-03 11:30:20,469] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-03 11:30:20,470] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-03 11:30:20,470] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-03 11:30:20,471] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,472] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:20,472] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:20,473] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:20,473] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-03 11:30:20,474] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-03 11:30:20,475] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-03 11:30:20,475] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-03 11:30:20,476] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-03 11:30:20,476] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-03 11:30:20,477] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,478] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:20,479] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:20,479] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,480] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:20,481] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:20,481] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:20,482] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:20,482] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:20,483] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:20,484] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-03 11:30:20,484] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-03 11:30:20,485] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-03 11:30:20,485] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-03 11:30:20,486] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-03 11:30:20,486] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-03 11:30:20,487] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:20,488] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:20,489] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:20,489] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:20,490] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:20,491] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:20,491] {spark_submit.py:495} INFO - 23/03/03 11:30:19 WARN Executor: Issue communicating with driver in heartbeater
[2023-03-03 11:30:20,492] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:20,492] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:20,493] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:20,494] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-03-03 11:30:20,494] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-03-03 11:30:20,495] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2023-03-03 11:30:20,496] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-03-03 11:30:20,496] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-03-03 11:30:20,498] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-03-03 11:30:20,500] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:20,502] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-03-03 11:30:20,503] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-03-03 11:30:20,504] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:20,505] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-03-03 11:30:20,505] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-03-03 11:30:20,509] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-03 11:30:20,512] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-03 11:30:20,513] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-03 11:30:20,515] {spark_submit.py:495} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:20,516] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:20,518] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:20,521] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-03 11:30:20,523] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-03 11:30:20,526] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-03 11:30:20,527] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-03 11:30:20,529] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-03 11:30:20,530] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-03 11:30:20,533] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-03 11:30:20,535] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-03 11:30:20,538] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-03 11:30:20,539] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:20,541] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:20,541] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:20,542] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:20,543] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:20,544] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:20,547] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-03 11:30:20,550] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:20,552] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@7f5e973bdd66:44831
[2023-03-03 11:30:20,554] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-03 11:30:20,555] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-03 11:30:20,558] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-03 11:30:20,559] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-03 11:30:20,560] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,560] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:20,561] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:20,561] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,562] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:20,563] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:20,563] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:20,565] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:20,566] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:20,567] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:20,568] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:20,569] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,570] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:20,570] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:20,572] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,573] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:20,574] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:20,575] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:20,576] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:20,577] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:20,578] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:20,578] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:20,579] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,579] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-03 11:30:20,580] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-03 11:30:20,581] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:20,582] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-03 11:30:20,583] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-03 11:30:20,583] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-03 11:30:20,584] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-03 11:30:20,585] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-03 11:30:20,585] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-03 11:30:20,586] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,587] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:20,588] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:20,590] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:20,591] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-03 11:30:20,592] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-03 11:30:20,594] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-03 11:30:20,595] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-03 11:30:20,595] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-03 11:30:20,596] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-03 11:30:20,597] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,597] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:20,598] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:20,599] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,600] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:20,601] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:20,601] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:20,602] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:20,603] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:20,603] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:20,604] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-03 11:30:20,604] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-03 11:30:20,606] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-03 11:30:20,606] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-03 11:30:20,609] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-03 11:30:20,612] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-03 11:30:20,613] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:20,614] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:20,615] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:20,615] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:20,616] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:20,617] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:20,618] {spark_submit.py:495} INFO - 23/03/03 11:30:20 INFO InMemoryFileIndex: It took 56 ms to list leaf files for 1 paths.
[2023-03-03 11:30:20,618] {spark_submit.py:495} INFO - 23/03/03 11:30:20 INFO Executor: Told to re-register on heartbeat
[2023-03-03 11:30:20,620] {spark_submit.py:495} INFO - 23/03/03 11:30:20 INFO BlockManager: BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None) re-registering with master
[2023-03-03 11:30:20,620] {spark_submit.py:495} INFO - 23/03/03 11:30:20 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None)
[2023-03-03 11:30:20,621] {spark_submit.py:495} INFO - 23/03/03 11:30:20 ERROR Inbox: Ignoring error
[2023-03-03 11:30:20,622] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:20,623] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:20,627] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:20,628] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-03 11:30:20,629] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-03 11:30:20,630] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-03 11:30:20,631] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-03 11:30:20,639] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-03 11:30:20,640] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-03 11:30:20,641] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-03 11:30:20,642] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-03 11:30:20,642] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-03 11:30:20,643] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:20,644] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:20,644] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:20,645] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:20,645] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:20,646] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:20,647] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-03 11:30:20,647] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-03 11:30:20,648] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-03 11:30:20,649] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-03 11:30:20,654] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@7f5e973bdd66:44831
[2023-03-03 11:30:20,655] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-03 11:30:20,656] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-03 11:30:20,657] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-03 11:30:20,657] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-03 11:30:20,658] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,659] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:20,660] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:20,661] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,662] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:20,663] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:20,663] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:20,664] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:20,664] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:20,665] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:20,666] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:20,666] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,667] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:20,668] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:20,668] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,669] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:20,670] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:20,670] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:20,671] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:20,671] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:20,672] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:20,673] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:20,673] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,675] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-03 11:30:20,676] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-03 11:30:20,677] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:20,679] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-03 11:30:20,680] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-03 11:30:20,681] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-03 11:30:20,682] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-03 11:30:20,683] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-03 11:30:20,683] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-03 11:30:20,684] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,684] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:20,685] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:20,686] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:20,687] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-03 11:30:20,688] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-03 11:30:20,688] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-03 11:30:20,689] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-03 11:30:20,689] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-03 11:30:20,690] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-03 11:30:20,691] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,692] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:20,692] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:20,694] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,695] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:20,696] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:20,698] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:20,699] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:20,700] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:20,701] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:20,701] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-03 11:30:20,702] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-03 11:30:20,703] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-03 11:30:20,703] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-03 11:30:20,704] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-03 11:30:20,704] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-03 11:30:20,705] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:20,706] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:20,706] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:20,707] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:20,708] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:20,709] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:20,709] {spark_submit.py:495} INFO - 23/03/03 11:30:20 WARN Executor: Issue communicating with driver in heartbeater
[2023-03-03 11:30:20,710] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:20,713] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:20,714] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:20,715] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-03-03 11:30:20,716] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-03-03 11:30:20,716] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2023-03-03 11:30:20,717] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-03-03 11:30:20,718] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-03-03 11:30:20,719] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-03-03 11:30:20,721] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:20,722] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-03-03 11:30:20,723] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-03-03 11:30:20,727] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:20,728] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-03-03 11:30:20,729] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-03-03 11:30:20,729] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-03 11:30:20,730] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-03 11:30:20,731] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-03 11:30:20,732] {spark_submit.py:495} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:20,739] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:20,741] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:20,742] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-03 11:30:20,743] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-03 11:30:20,744] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-03 11:30:20,745] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-03 11:30:20,746] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-03 11:30:20,747] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-03 11:30:20,749] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-03 11:30:20,750] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-03 11:30:20,751] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-03 11:30:20,752] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:20,754] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:20,756] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:20,758] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:20,760] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:20,761] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:20,763] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-03 11:30:20,764] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:20,765] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@7f5e973bdd66:44831
[2023-03-03 11:30:20,765] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-03 11:30:20,766] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-03 11:30:20,767] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-03 11:30:20,768] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-03 11:30:20,768] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,769] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:20,770] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:20,770] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,771] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:20,772] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:20,773] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:20,773] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:20,774] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:20,775] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:20,775] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:20,776] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,776] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:20,779] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:20,781] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,782] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:20,783] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:20,783] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:20,784] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:20,784] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:20,785] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:20,785] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:20,786] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,787] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-03 11:30:20,787] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-03 11:30:20,788] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:20,788] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-03 11:30:20,789] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-03 11:30:20,790] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-03 11:30:20,792] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-03 11:30:20,793] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-03 11:30:20,794] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-03 11:30:20,795] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,797] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:20,799] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:20,803] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:20,805] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-03 11:30:20,806] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-03 11:30:20,806] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-03 11:30:20,807] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-03 11:30:20,808] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-03 11:30:20,808] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-03 11:30:20,809] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,810] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:20,810] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:20,811] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,813] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:20,816] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:20,822] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:20,823] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:20,824] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:20,825] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:20,826] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-03 11:30:20,827] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-03 11:30:20,827] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-03 11:30:20,828] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-03 11:30:20,829] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-03 11:30:20,830] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-03 11:30:20,830] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:20,833] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:20,835] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:20,836] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:20,838] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:20,840] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:20,840] {spark_submit.py:495} INFO - 23/03/03 11:30:20 INFO Executor: Told to re-register on heartbeat
[2023-03-03 11:30:20,841] {spark_submit.py:495} INFO - 23/03/03 11:30:20 INFO BlockManager: BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None) re-registering with master
[2023-03-03 11:30:20,842] {spark_submit.py:495} INFO - 23/03/03 11:30:20 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None)
[2023-03-03 11:30:20,842] {spark_submit.py:495} INFO - 23/03/03 11:30:20 ERROR Inbox: Ignoring error
[2023-03-03 11:30:20,843] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:20,844] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:20,844] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:20,845] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-03 11:30:20,846] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-03 11:30:20,846] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-03 11:30:20,847] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-03 11:30:20,848] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-03 11:30:20,850] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-03 11:30:20,852] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-03 11:30:20,853] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-03 11:30:20,855] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-03 11:30:20,856] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:20,858] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:20,858] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:20,859] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:20,859] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:20,860] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:20,861] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-03 11:30:20,861] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-03 11:30:20,862] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-03 11:30:20,863] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-03 11:30:20,863] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@7f5e973bdd66:44831
[2023-03-03 11:30:20,864] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-03 11:30:20,865] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-03 11:30:20,865] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-03 11:30:20,866] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-03 11:30:20,866] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,867] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:20,868] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:20,870] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,872] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:20,873] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:20,874] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:20,874] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:20,875] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:20,876] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:20,877] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:20,877] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,878] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:20,878] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:20,879] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,879] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:20,880] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:20,881] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:20,882] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:20,882] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:20,883] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:20,884] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:20,884] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,885] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-03 11:30:20,886] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-03 11:30:20,887] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:20,888] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-03 11:30:20,888] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-03 11:30:20,889] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-03 11:30:20,893] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-03 11:30:20,893] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-03 11:30:20,894] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-03 11:30:20,894] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,895] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:20,896] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:20,896] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:20,897] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-03 11:30:20,897] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-03 11:30:20,898] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-03 11:30:20,899] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-03 11:30:20,899] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-03 11:30:20,900] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-03 11:30:20,900] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,901] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:20,901] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:20,902] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,902] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:20,904] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:20,906] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:20,907] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:20,908] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:20,909] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:20,910] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-03 11:30:20,910] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-03 11:30:20,911] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-03 11:30:20,912] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-03 11:30:20,912] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-03 11:30:20,913] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-03 11:30:20,913] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:20,914] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:20,914] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:20,915] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:20,916] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:20,918] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:20,920] {spark_submit.py:495} INFO - 23/03/03 11:30:20 WARN Executor: Issue communicating with driver in heartbeater
[2023-03-03 11:30:20,921] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:20,922] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:20,923] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:20,924] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-03-03 11:30:20,924] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-03-03 11:30:20,925] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2023-03-03 11:30:20,925] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-03-03 11:30:20,926] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-03-03 11:30:20,926] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-03-03 11:30:20,927] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:20,928] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-03-03 11:30:20,929] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-03-03 11:30:20,929] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:20,930] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-03-03 11:30:20,931] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-03-03 11:30:20,932] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-03 11:30:20,933] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-03 11:30:20,934] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-03 11:30:20,935] {spark_submit.py:495} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:20,936] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:20,937] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:20,938] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-03 11:30:20,939] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-03 11:30:20,940] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-03 11:30:20,942] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-03 11:30:20,943] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-03 11:30:20,944] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-03 11:30:20,944] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-03 11:30:20,945] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-03 11:30:20,945] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-03 11:30:20,946] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:20,947] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:20,947] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:20,948] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:20,949] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:20,949] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:20,950] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-03 11:30:20,951] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:20,951] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@7f5e973bdd66:44831
[2023-03-03 11:30:20,952] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-03 11:30:20,953] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-03 11:30:20,953] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-03 11:30:20,954] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-03 11:30:20,954] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,955] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:20,956] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:20,956] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,957] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:20,958] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:20,958] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:20,959] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:20,959] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:20,960] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:20,962] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:20,963] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,964] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:20,965] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:20,966] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,967] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:20,967] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:20,968] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:20,968] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:20,969] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:20,970] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:20,970] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:20,971] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,971] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-03 11:30:20,972] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-03 11:30:20,973] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:20,973] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-03 11:30:20,974] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-03 11:30:20,975] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-03 11:30:20,975] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-03 11:30:20,976] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-03 11:30:20,976] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-03 11:30:20,977] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,978] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:20,978] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:20,979] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:20,980] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-03 11:30:20,982] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-03 11:30:20,983] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-03 11:30:20,987] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-03 11:30:20,988] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-03 11:30:20,988] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-03 11:30:20,989] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:20,990] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:20,990] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:20,991] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:20,992] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:20,992] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:20,993] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:20,993] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:20,994] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:20,995] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:20,995] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-03 11:30:20,997] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-03 11:30:20,998] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-03 11:30:21,000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-03 11:30:21,001] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-03 11:30:21,001] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-03 11:30:21,002] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:21,003] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:21,003] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:21,004] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:21,005] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:21,005] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:21,006] {spark_submit.py:495} INFO - 23/03/03 11:30:20 INFO Executor: Told to re-register on heartbeat
[2023-03-03 11:30:21,007] {spark_submit.py:495} INFO - 23/03/03 11:30:20 INFO BlockManager: BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None) re-registering with master
[2023-03-03 11:30:21,007] {spark_submit.py:495} INFO - 23/03/03 11:30:20 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None)
[2023-03-03 11:30:21,008] {spark_submit.py:495} INFO - 23/03/03 11:30:20 ERROR Inbox: Ignoring error
[2023-03-03 11:30:21,008] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:21,009] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:21,009] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:21,010] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-03 11:30:21,010] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-03 11:30:21,011] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-03 11:30:21,012] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-03 11:30:21,016] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-03 11:30:21,017] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-03 11:30:21,018] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-03 11:30:21,019] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-03 11:30:21,020] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-03 11:30:21,021] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:21,022] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:21,022] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:21,023] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:21,025] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:21,025] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:21,026] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-03 11:30:21,027] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-03 11:30:21,027] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-03 11:30:21,028] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-03 11:30:21,028] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@7f5e973bdd66:44831
[2023-03-03 11:30:21,029] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-03 11:30:21,030] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-03 11:30:21,031] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-03 11:30:21,032] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-03 11:30:21,034] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,035] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:21,036] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:21,038] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,039] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,040] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,041] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,042] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:21,044] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:21,045] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:21,046] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:21,046] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,047] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:21,048] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:21,049] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,050] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,052] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,054] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,055] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:21,055] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:21,057] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:21,058] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:21,060] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,061] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-03 11:30:21,062] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-03 11:30:21,062] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:21,063] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-03 11:30:21,064] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-03 11:30:21,064] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-03 11:30:21,065] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-03 11:30:21,065] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-03 11:30:21,066] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-03 11:30:21,066] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,067] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,068] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,068] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,069] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-03 11:30:21,070] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-03 11:30:21,071] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-03 11:30:21,073] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-03 11:30:21,074] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-03 11:30:21,076] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-03 11:30:21,077] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,083] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:21,084] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:21,085] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,086] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,086] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,087] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,087] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:21,088] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:21,089] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:21,090] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-03 11:30:21,090] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-03 11:30:21,091] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-03 11:30:21,092] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-03 11:30:21,093] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-03 11:30:21,094] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-03 11:30:21,095] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:21,095] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:21,096] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:21,096] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:21,097] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:21,097] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:21,098] {spark_submit.py:495} INFO - 23/03/03 11:30:20 WARN Executor: Issue communicating with driver in heartbeater
[2023-03-03 11:30:21,099] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:21,099] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:21,100] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:21,100] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-03-03 11:30:21,101] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-03-03 11:30:21,101] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2023-03-03 11:30:21,102] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-03-03 11:30:21,103] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-03-03 11:30:21,103] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-03-03 11:30:21,104] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:21,104] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-03-03 11:30:21,105] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-03-03 11:30:21,105] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:21,106] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-03-03 11:30:21,106] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-03-03 11:30:21,107] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-03 11:30:21,108] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-03 11:30:21,109] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-03 11:30:21,110] {spark_submit.py:495} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:21,111] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:21,112] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:21,112] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-03 11:30:21,113] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-03 11:30:21,113] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-03 11:30:21,114] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-03 11:30:21,114] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-03 11:30:21,115] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-03 11:30:21,116] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-03 11:30:21,116] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-03 11:30:21,117] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-03 11:30:21,118] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:21,118] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:21,119] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:21,120] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:21,121] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:21,122] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:21,124] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-03 11:30:21,125] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:21,126] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@7f5e973bdd66:44831
[2023-03-03 11:30:21,127] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-03 11:30:21,127] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-03 11:30:21,128] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-03 11:30:21,131] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-03 11:30:21,132] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,133] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:21,133] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:21,134] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,134] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,135] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,136] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,136] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:21,137] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:21,137] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:21,138] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:21,139] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,139] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:21,140] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:21,141] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,141] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,142] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,143] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,143] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:21,144] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:21,144] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:21,145] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:21,146] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,148] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-03 11:30:21,149] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-03 11:30:21,151] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:21,151] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-03 11:30:21,152] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-03 11:30:21,153] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-03 11:30:21,154] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-03 11:30:21,154] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-03 11:30:21,155] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-03 11:30:21,156] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,156] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,157] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,157] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,158] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-03 11:30:21,158] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-03 11:30:21,159] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-03 11:30:21,160] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-03 11:30:21,160] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-03 11:30:21,161] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-03 11:30:21,162] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,162] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:21,163] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:21,164] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,166] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,167] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,169] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,169] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:21,170] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:21,171] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:21,171] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-03 11:30:21,172] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-03 11:30:21,173] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-03 11:30:21,174] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-03 11:30:21,174] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-03 11:30:21,175] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-03 11:30:21,176] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:21,176] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:21,177] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:21,178] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:21,178] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:21,179] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:21,179] {spark_submit.py:495} INFO - 23/03/03 11:30:20 INFO Executor: Told to re-register on heartbeat
[2023-03-03 11:30:21,180] {spark_submit.py:495} INFO - 23/03/03 11:30:20 INFO BlockManager: BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None) re-registering with master
[2023-03-03 11:30:21,181] {spark_submit.py:495} INFO - 23/03/03 11:30:20 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None)
[2023-03-03 11:30:21,181] {spark_submit.py:495} INFO - 23/03/03 11:30:20 WARN Executor: Issue communicating with driver in heartbeater
[2023-03-03 11:30:21,182] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:21,183] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:21,184] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:21,185] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-03-03 11:30:21,186] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-03-03 11:30:21,188] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2023-03-03 11:30:21,189] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-03-03 11:30:21,190] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-03-03 11:30:21,191] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-03-03 11:30:21,191] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:21,192] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-03-03 11:30:21,193] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-03-03 11:30:21,193] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:21,194] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-03-03 11:30:21,194] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-03-03 11:30:21,195] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-03 11:30:21,196] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-03 11:30:21,196] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-03 11:30:21,197] {spark_submit.py:495} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:21,197] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:21,198] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:21,199] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-03 11:30:21,199] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-03 11:30:21,200] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-03 11:30:21,200] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-03 11:30:21,201] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-03 11:30:21,202] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-03 11:30:21,203] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-03 11:30:21,203] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-03 11:30:21,204] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-03 11:30:21,205] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:21,206] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:21,207] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:21,208] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:21,209] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:21,209] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:21,210] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-03 11:30:21,211] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:21,211] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@7f5e973bdd66:44831
[2023-03-03 11:30:21,212] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-03 11:30:21,213] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-03 11:30:21,214] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-03 11:30:21,214] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-03 11:30:21,215] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,216] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:21,216] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:21,217] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,217] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,218] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,219] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,219] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:21,220] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:21,220] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:21,221] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:21,221] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,222] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:21,223] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:21,224] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,224] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,225] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,226] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,226] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:21,227] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:21,227] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:21,228] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:21,228] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,229] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-03 11:30:21,230] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-03 11:30:21,230] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:21,231] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-03 11:30:21,231] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-03 11:30:21,232] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-03 11:30:21,233] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-03 11:30:21,233] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-03 11:30:21,233] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-03 11:30:21,234] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,235] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,235] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,236] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,236] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-03 11:30:21,237] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-03 11:30:21,237] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-03 11:30:21,238] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-03 11:30:21,238] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-03 11:30:21,239] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-03 11:30:21,239] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,240] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:21,240] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:21,241] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,241] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,243] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,243] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,244] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:21,245] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:21,246] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:21,246] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-03 11:30:21,247] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-03 11:30:21,247] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-03 11:30:21,248] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-03 11:30:21,248] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-03 11:30:21,249] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-03 11:30:21,249] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:21,250] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:21,251] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:21,251] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:21,252] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:21,253] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:21,253] {spark_submit.py:495} INFO - 23/03/03 11:30:20 ERROR Inbox: Ignoring error
[2023-03-03 11:30:21,254] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:21,255] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:21,255] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:21,256] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-03 11:30:21,256] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-03 11:30:21,257] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-03 11:30:21,257] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-03 11:30:21,258] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-03 11:30:21,259] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-03 11:30:21,259] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-03 11:30:21,260] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-03 11:30:21,260] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-03 11:30:21,261] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:21,261] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:21,262] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:21,263] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:21,263] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:21,264] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:21,264] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-03 11:30:21,265] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-03 11:30:21,266] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-03 11:30:21,267] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-03 11:30:21,267] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@7f5e973bdd66:44831
[2023-03-03 11:30:21,268] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-03 11:30:21,268] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-03 11:30:21,269] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-03 11:30:21,269] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-03 11:30:21,270] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,270] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:21,271] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:21,271] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,272] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,273] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,273] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,274] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:21,275] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:21,275] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:21,276] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:21,277] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,277] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:21,278] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:21,278] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,279] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,279] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,280] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,281] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:21,281] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:21,282] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:21,282] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:21,283] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,284] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-03 11:30:21,284] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-03 11:30:21,285] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:21,285] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-03 11:30:21,286] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-03 11:30:21,287] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-03 11:30:21,287] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-03 11:30:21,288] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-03 11:30:21,288] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-03 11:30:21,289] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,289] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,290] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,291] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,291] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-03 11:30:21,292] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-03 11:30:21,292] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-03 11:30:21,293] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-03 11:30:21,293] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-03 11:30:21,294] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-03 11:30:21,295] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,295] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:21,296] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:21,296] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,297] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,298] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,298] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,299] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:21,299] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:21,300] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:21,300] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-03 11:30:21,301] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-03 11:30:21,301] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-03 11:30:21,302] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-03 11:30:21,302] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-03 11:30:21,303] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-03 11:30:21,304] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:21,304] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:21,305] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:21,306] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:21,306] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:21,307] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:21,307] {spark_submit.py:495} INFO - 23/03/03 11:30:20 INFO Executor: Told to re-register on heartbeat
[2023-03-03 11:30:21,308] {spark_submit.py:495} INFO - 23/03/03 11:30:20 INFO BlockManager: BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None) re-registering with master
[2023-03-03 11:30:21,309] {spark_submit.py:495} INFO - 23/03/03 11:30:20 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None)
[2023-03-03 11:30:21,309] {spark_submit.py:495} INFO - 23/03/03 11:30:20 WARN Executor: Issue communicating with driver in heartbeater
[2023-03-03 11:30:21,310] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:21,311] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:21,311] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:21,312] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-03-03 11:30:21,313] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-03-03 11:30:21,313] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2023-03-03 11:30:21,314] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-03-03 11:30:21,315] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-03-03 11:30:21,315] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-03-03 11:30:21,316] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:21,316] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-03-03 11:30:21,317] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-03-03 11:30:21,318] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:21,318] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-03-03 11:30:21,319] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-03-03 11:30:21,320] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-03 11:30:21,320] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-03 11:30:21,321] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-03 11:30:21,322] {spark_submit.py:495} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:21,323] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:21,323] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:21,324] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-03 11:30:21,325] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-03 11:30:21,325] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-03 11:30:21,326] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-03 11:30:21,327] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-03 11:30:21,327] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-03 11:30:21,328] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-03 11:30:21,328] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-03 11:30:21,329] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-03 11:30:21,329] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:21,330] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:21,331] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:21,331] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:21,332] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:21,333] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:21,333] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-03 11:30:21,334] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:21,334] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@7f5e973bdd66:44831
[2023-03-03 11:30:21,335] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-03 11:30:21,336] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-03 11:30:21,336] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-03 11:30:21,337] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-03 11:30:21,338] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,338] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:21,339] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:21,339] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,340] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,341] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,342] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,342] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:21,343] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:21,344] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:21,345] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:21,345] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,346] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:21,346] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:21,347] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,347] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,348] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,349] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,349] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:21,350] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:21,350] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:21,351] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:21,352] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,352] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-03 11:30:21,353] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-03 11:30:21,353] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:21,354] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-03 11:30:21,354] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-03 11:30:21,355] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-03 11:30:21,356] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-03 11:30:21,356] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-03 11:30:21,357] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-03 11:30:21,357] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,358] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,359] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,359] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,360] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-03 11:30:21,360] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-03 11:30:21,361] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-03 11:30:21,362] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-03 11:30:21,362] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-03 11:30:21,363] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-03 11:30:21,364] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,364] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:21,365] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:21,366] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,366] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,367] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,367] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,368] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:21,369] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:21,369] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:21,370] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-03 11:30:21,370] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-03 11:30:21,371] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-03 11:30:21,371] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-03 11:30:21,372] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-03 11:30:21,372] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-03 11:30:21,373] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:21,374] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:21,374] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:21,375] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:21,375] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:21,376] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:21,376] {spark_submit.py:495} INFO - 23/03/03 11:30:20 ERROR Inbox: Ignoring error
[2023-03-03 11:30:21,377] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:21,378] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:21,378] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:21,379] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-03 11:30:21,379] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-03 11:30:21,380] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-03 11:30:21,380] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-03 11:30:21,381] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-03 11:30:21,382] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-03 11:30:21,383] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-03 11:30:21,383] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-03 11:30:21,384] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-03 11:30:21,384] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:21,385] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:21,386] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:21,386] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:21,387] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:21,387] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:21,388] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-03 11:30:21,388] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-03 11:30:21,389] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-03 11:30:21,390] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-03 11:30:21,390] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@7f5e973bdd66:44831
[2023-03-03 11:30:21,391] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-03 11:30:21,391] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-03 11:30:21,392] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-03 11:30:21,392] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-03 11:30:21,393] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,394] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:21,394] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:21,395] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,395] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,396] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,396] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,397] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:21,398] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:21,398] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:21,399] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:21,399] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,400] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:21,401] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:21,402] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,402] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,403] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,403] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,404] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:21,405] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:21,405] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:21,406] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:21,406] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,407] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-03 11:30:21,408] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-03 11:30:21,408] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:21,409] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-03 11:30:21,410] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-03 11:30:21,410] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-03 11:30:21,411] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-03 11:30:21,412] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-03 11:30:21,413] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-03 11:30:21,413] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,414] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,414] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,415] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,416] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-03 11:30:21,416] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-03 11:30:21,417] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-03 11:30:21,418] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-03 11:30:21,419] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-03 11:30:21,419] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-03 11:30:21,420] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,420] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:21,421] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:21,422] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,422] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,423] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,423] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,424] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:21,424] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:21,425] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:21,425] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-03 11:30:21,426] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-03 11:30:21,427] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-03 11:30:21,427] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-03 11:30:21,428] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-03 11:30:21,429] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-03 11:30:21,429] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:21,430] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:21,431] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:21,431] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:21,432] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:21,432] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:21,433] {spark_submit.py:495} INFO - 23/03/03 11:30:20 INFO Executor: Told to re-register on heartbeat
[2023-03-03 11:30:21,433] {spark_submit.py:495} INFO - 23/03/03 11:30:20 INFO BlockManager: BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None) re-registering with master
[2023-03-03 11:30:21,434] {spark_submit.py:495} INFO - 23/03/03 11:30:20 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None)
[2023-03-03 11:30:21,435] {spark_submit.py:495} INFO - 23/03/03 11:30:20 ERROR Inbox: Ignoring error
[2023-03-03 11:30:21,436] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:21,437] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:21,438] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:21,439] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-03 11:30:21,440] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-03 11:30:21,441] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-03 11:30:21,442] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-03 11:30:21,442] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-03 11:30:21,443] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-03 11:30:21,444] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-03 11:30:21,444] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-03 11:30:21,445] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-03 11:30:21,445] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:21,446] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:21,447] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:21,447] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:21,448] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:21,448] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:21,449] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-03 11:30:21,450] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-03 11:30:21,450] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-03 11:30:21,451] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-03 11:30:21,451] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@7f5e973bdd66:44831
[2023-03-03 11:30:21,452] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-03 11:30:21,452] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-03 11:30:21,453] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-03 11:30:21,454] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-03 11:30:21,454] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,455] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:21,455] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:21,456] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,456] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,457] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,457] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,458] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:21,458] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:21,459] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:21,460] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:21,460] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,461] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:21,461] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:21,462] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,463] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,464] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,465] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,465] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:21,466] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:21,467] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:21,467] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:21,468] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,469] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-03 11:30:21,469] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-03 11:30:21,470] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:21,470] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-03 11:30:21,471] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-03 11:30:21,471] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-03 11:30:21,472] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-03 11:30:21,473] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-03 11:30:21,473] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-03 11:30:21,474] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,475] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,475] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,476] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,476] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-03 11:30:21,477] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-03 11:30:21,478] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-03 11:30:21,479] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-03 11:30:21,479] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-03 11:30:21,480] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-03 11:30:21,481] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,481] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:21,482] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:21,482] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,483] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,483] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,484] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,484] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:21,485] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:21,486] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:21,486] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-03 11:30:21,487] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-03 11:30:21,488] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-03 11:30:21,488] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-03 11:30:21,489] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-03 11:30:21,489] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-03 11:30:21,490] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:21,490] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:21,491] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:21,491] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:21,492] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:21,492] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:21,493] {spark_submit.py:495} INFO - 23/03/03 11:30:20 WARN Executor: Issue communicating with driver in heartbeater
[2023-03-03 11:30:21,494] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:21,494] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:21,495] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:21,495] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-03-03 11:30:21,496] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-03-03 11:30:21,497] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2023-03-03 11:30:21,497] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-03-03 11:30:21,498] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-03-03 11:30:21,498] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-03-03 11:30:21,499] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:21,500] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-03-03 11:30:21,502] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-03-03 11:30:21,503] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:21,504] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-03-03 11:30:21,505] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-03-03 11:30:21,505] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-03 11:30:21,506] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-03 11:30:21,507] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-03 11:30:21,507] {spark_submit.py:495} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:21,508] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:21,508] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:21,509] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-03 11:30:21,510] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-03 11:30:21,510] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-03 11:30:21,511] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-03 11:30:21,511] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-03 11:30:21,512] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-03 11:30:21,512] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-03 11:30:21,513] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-03 11:30:21,513] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-03 11:30:21,514] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:21,514] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:21,515] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:21,516] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:21,516] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:21,517] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:21,517] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-03 11:30:21,518] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:21,518] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@7f5e973bdd66:44831
[2023-03-03 11:30:21,519] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-03 11:30:21,520] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-03 11:30:21,521] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-03 11:30:21,521] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-03 11:30:21,522] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,523] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:21,524] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:21,524] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,525] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,526] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,526] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,527] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:21,528] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:21,528] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:21,529] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:21,529] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,530] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:21,531] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:21,531] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,532] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,533] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,533] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,534] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:21,535] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:21,535] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:21,536] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:21,537] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,538] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-03 11:30:21,539] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-03 11:30:21,539] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:21,540] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-03 11:30:21,540] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-03 11:30:21,541] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-03 11:30:21,542] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-03 11:30:21,542] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-03 11:30:21,543] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-03 11:30:21,543] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,544] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,545] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,545] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,546] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-03 11:30:21,546] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-03 11:30:21,547] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-03 11:30:21,547] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-03 11:30:21,548] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-03 11:30:21,548] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-03 11:30:21,549] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,549] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:21,550] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:21,551] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,552] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,553] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,554] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,554] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:21,555] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:21,556] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:21,557] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-03 11:30:21,558] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-03 11:30:21,558] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-03 11:30:21,559] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-03 11:30:21,560] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-03 11:30:21,560] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-03 11:30:21,561] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:21,561] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:21,562] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:21,562] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:21,563] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:21,564] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:21,564] {spark_submit.py:495} INFO - 23/03/03 11:30:21 INFO Executor: Told to re-register on heartbeat
[2023-03-03 11:30:21,565] {spark_submit.py:495} INFO - 23/03/03 11:30:21 INFO BlockManager: BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None) re-registering with master
[2023-03-03 11:30:21,566] {spark_submit.py:495} INFO - 23/03/03 11:30:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None)
[2023-03-03 11:30:21,567] {spark_submit.py:495} INFO - 23/03/03 11:30:21 ERROR Inbox: Ignoring error
[2023-03-03 11:30:21,567] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:21,568] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:21,569] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:21,569] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-03 11:30:21,570] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-03 11:30:21,570] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-03 11:30:21,571] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-03 11:30:21,572] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-03 11:30:21,572] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-03 11:30:21,573] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-03 11:30:21,574] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-03 11:30:21,575] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-03 11:30:21,576] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:21,576] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:21,577] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:21,578] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:21,578] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:21,579] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:21,579] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-03 11:30:21,580] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-03 11:30:21,581] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-03 11:30:21,581] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-03 11:30:21,582] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@7f5e973bdd66:44831
[2023-03-03 11:30:21,583] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-03 11:30:21,583] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-03 11:30:21,584] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-03 11:30:21,585] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-03 11:30:21,585] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,586] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:21,587] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:21,587] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,588] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2023-03-03 11:30:21,589] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2023-03-03 11:30:21,589] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2023-03-03 11:30:21,590] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2023-03-03 11:30:21,590] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2023-03-03 11:30:21,591] {spark_submit.py:495} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2023-03-03 11:30:21,591] {spark_submit.py:495} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2023-03-03 11:30:21,592] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2023-03-03 11:30:21,592] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2023-03-03 11:30:21,593] {spark_submit.py:495} INFO - ... 19 more
[2023-03-03 11:30:21,593] {spark_submit.py:495} INFO - 23/03/03 11:30:21 WARN Executor: Issue communicating with driver in heartbeater
[2023-03-03 11:30:21,594] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:21,595] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:21,595] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:21,596] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-03-03 11:30:21,597] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-03-03 11:30:21,598] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2023-03-03 11:30:21,599] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-03-03 11:30:21,600] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-03-03 11:30:21,601] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-03-03 11:30:21,602] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:21,603] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-03-03 11:30:21,603] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-03-03 11:30:21,604] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:21,604] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-03-03 11:30:21,605] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-03-03 11:30:21,606] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-03 11:30:21,606] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-03 11:30:21,607] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-03 11:30:21,607] {spark_submit.py:495} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:21,608] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:21,608] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:21,609] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-03 11:30:21,610] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-03 11:30:21,611] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-03 11:30:21,611] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-03 11:30:21,612] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-03 11:30:21,612] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-03 11:30:21,613] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-03 11:30:21,614] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-03 11:30:21,615] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-03 11:30:21,616] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:21,617] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:21,618] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:21,619] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:21,620] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:21,621] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:21,622] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-03 11:30:21,622] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:21,623] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@7f5e973bdd66:44831
[2023-03-03 11:30:21,624] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-03 11:30:21,624] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-03 11:30:21,625] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-03 11:30:21,625] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-03 11:30:21,626] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,627] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:21,627] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:21,628] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,629] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2023-03-03 11:30:21,629] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2023-03-03 11:30:21,630] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2023-03-03 11:30:21,630] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2023-03-03 11:30:21,631] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2023-03-03 11:30:21,632] {spark_submit.py:495} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2023-03-03 11:30:21,632] {spark_submit.py:495} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2023-03-03 11:30:21,633] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2023-03-03 11:30:21,634] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2023-03-03 11:30:21,634] {spark_submit.py:495} INFO - ... 19 more
[2023-03-03 11:30:21,635] {spark_submit.py:495} INFO - 23/03/03 11:30:21 INFO Executor: Told to re-register on heartbeat
[2023-03-03 11:30:21,635] {spark_submit.py:495} INFO - 23/03/03 11:30:21 INFO BlockManager: BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None) re-registering with master
[2023-03-03 11:30:21,636] {spark_submit.py:495} INFO - 23/03/03 11:30:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None)
[2023-03-03 11:30:21,636] {spark_submit.py:495} INFO - 23/03/03 11:30:21 WARN Executor: Issue communicating with driver in heartbeater
[2023-03-03 11:30:21,637] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:21,637] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:21,638] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:21,639] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-03-03 11:30:21,639] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-03-03 11:30:21,640] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2023-03-03 11:30:21,641] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-03-03 11:30:21,642] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-03-03 11:30:21,642] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-03-03 11:30:21,643] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:21,643] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-03-03 11:30:21,644] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-03-03 11:30:21,645] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:21,645] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-03-03 11:30:21,646] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-03-03 11:30:21,646] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-03 11:30:21,647] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-03 11:30:21,647] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-03 11:30:21,648] {spark_submit.py:495} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:21,648] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:21,649] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:21,649] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-03 11:30:21,650] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-03 11:30:21,650] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-03 11:30:21,651] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-03 11:30:21,652] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-03 11:30:21,652] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-03 11:30:21,653] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-03 11:30:21,653] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-03 11:30:21,654] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-03 11:30:21,654] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:21,655] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:21,655] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:21,656] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:21,656] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:21,657] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:21,657] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-03 11:30:21,658] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:21,658] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@7f5e973bdd66:44831
[2023-03-03 11:30:21,659] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-03 11:30:21,659] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-03 11:30:21,660] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-03 11:30:21,660] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-03 11:30:21,661] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,662] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:21,662] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:21,663] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,663] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,664] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,664] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,665] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:21,665] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:21,666] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:21,666] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:21,667] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,667] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:21,668] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:21,668] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,669] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,670] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,670] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,671] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:21,671] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:21,672] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:21,673] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:21,674] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,674] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-03 11:30:21,675] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-03 11:30:21,676] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:21,677] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-03 11:30:21,677] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-03 11:30:21,678] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-03 11:30:21,679] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-03 11:30:21,679] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-03 11:30:21,680] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-03 11:30:21,681] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,681] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,682] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,682] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,683] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-03 11:30:21,683] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-03 11:30:21,684] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-03 11:30:21,685] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-03 11:30:21,685] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-03 11:30:21,686] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-03 11:30:21,686] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,687] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:21,687] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:21,688] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,689] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,689] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,690] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,690] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:21,691] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:21,692] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:21,693] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-03 11:30:21,694] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-03 11:30:21,696] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-03 11:30:21,697] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-03 11:30:21,698] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-03 11:30:21,699] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-03 11:30:21,700] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:21,700] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:21,701] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:21,702] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:21,702] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:21,703] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:21,704] {spark_submit.py:495} INFO - 23/03/03 11:30:21 ERROR Inbox: Ignoring error
[2023-03-03 11:30:21,704] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:21,705] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:21,705] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:21,706] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-03 11:30:21,707] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-03 11:30:21,707] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-03 11:30:21,708] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-03 11:30:21,709] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-03 11:30:21,709] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-03 11:30:21,710] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-03 11:30:21,711] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-03 11:30:21,712] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-03 11:30:21,712] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:21,713] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:21,714] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:21,714] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:21,715] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:21,715] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:21,716] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-03 11:30:21,717] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-03 11:30:21,717] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-03 11:30:21,718] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-03 11:30:21,719] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@7f5e973bdd66:44831
[2023-03-03 11:30:21,720] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-03 11:30:21,721] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-03 11:30:21,722] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-03 11:30:21,723] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-03 11:30:21,724] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,725] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:21,726] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:21,726] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,727] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,727] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,728] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,729] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:21,729] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:21,730] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:21,730] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:21,731] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,732] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:21,732] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:21,733] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,734] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,734] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,735] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,735] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:21,736] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:21,737] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:21,738] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:21,738] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,739] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-03 11:30:21,740] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-03 11:30:21,740] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:21,741] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-03 11:30:21,742] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-03 11:30:21,743] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-03 11:30:21,744] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-03 11:30:21,745] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-03 11:30:21,746] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-03 11:30:21,747] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,748] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,749] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,749] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,750] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-03 11:30:21,750] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-03 11:30:21,751] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-03 11:30:21,751] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-03 11:30:21,752] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-03 11:30:21,752] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-03 11:30:21,753] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,753] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:21,754] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:21,755] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,755] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,756] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,757] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,757] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:21,758] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:21,758] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:21,759] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-03 11:30:21,759] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-03 11:30:21,760] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-03 11:30:21,760] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-03 11:30:21,761] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-03 11:30:21,761] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-03 11:30:21,762] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:21,763] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:21,763] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:21,764] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:21,764] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:21,765] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:21,766] {spark_submit.py:495} INFO - 23/03/03 11:30:21 INFO Executor: Told to re-register on heartbeat
[2023-03-03 11:30:21,766] {spark_submit.py:495} INFO - 23/03/03 11:30:21 INFO BlockManager: BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None) re-registering with master
[2023-03-03 11:30:21,767] {spark_submit.py:495} INFO - 23/03/03 11:30:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None)
[2023-03-03 11:30:21,768] {spark_submit.py:495} INFO - 23/03/03 11:30:21 ERROR Inbox: Ignoring error
[2023-03-03 11:30:21,769] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:21,769] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:21,771] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:21,772] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-03 11:30:21,773] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-03 11:30:21,773] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-03 11:30:21,774] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-03 11:30:21,774] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-03 11:30:21,775] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-03 11:30:21,775] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-03 11:30:21,776] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-03 11:30:21,776] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-03 11:30:21,777] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:21,778] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:21,778] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:21,779] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:21,779] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:21,780] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:21,780] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-03 11:30:21,781] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-03 11:30:21,781] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-03 11:30:21,782] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-03 11:30:21,783] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@7f5e973bdd66:44831
[2023-03-03 11:30:21,783] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-03 11:30:21,784] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-03 11:30:21,784] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-03 11:30:21,785] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-03 11:30:21,786] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,786] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:21,787] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:21,789] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,790] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,791] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,792] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,793] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:21,794] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:21,795] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:21,796] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:21,797] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,797] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:21,798] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:21,798] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,799] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,800] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,800] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,801] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:21,802] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:21,802] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:21,803] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:21,804] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,804] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-03 11:30:21,805] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-03 11:30:21,805] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:21,806] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-03 11:30:21,807] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-03 11:30:21,807] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-03 11:30:21,808] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-03 11:30:21,809] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-03 11:30:21,809] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-03 11:30:21,810] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,811] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,811] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,812] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,813] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-03 11:30:21,813] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-03 11:30:21,815] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-03 11:30:21,816] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-03 11:30:21,817] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-03 11:30:21,818] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-03 11:30:21,819] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,820] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:21,821] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:21,822] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,823] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,824] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,826] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,828] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:21,829] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:21,830] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:21,831] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-03 11:30:21,831] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-03 11:30:21,832] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-03 11:30:21,833] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-03 11:30:21,833] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-03 11:30:21,834] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-03 11:30:21,834] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:21,835] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:21,835] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:21,836] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:21,836] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:21,837] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:21,837] {spark_submit.py:495} INFO - 23/03/03 11:30:21 WARN Executor: Issue communicating with driver in heartbeater
[2023-03-03 11:30:21,838] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:21,838] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:21,839] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:21,839] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-03-03 11:30:21,840] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-03-03 11:30:21,841] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2023-03-03 11:30:21,844] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-03-03 11:30:21,845] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-03-03 11:30:21,846] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-03-03 11:30:21,848] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:21,849] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-03-03 11:30:21,850] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-03-03 11:30:21,850] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:21,851] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-03-03 11:30:21,851] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-03-03 11:30:21,852] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-03 11:30:21,853] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-03 11:30:21,853] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-03 11:30:21,854] {spark_submit.py:495} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:30:21,854] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:30:21,855] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:30:21,856] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-03 11:30:21,857] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-03 11:30:21,857] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-03 11:30:21,858] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-03 11:30:21,859] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-03 11:30:21,860] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-03 11:30:21,860] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-03 11:30:21,861] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-03 11:30:21,862] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-03 11:30:21,863] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:21,864] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:21,865] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:21,865] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:21,866] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:21,866] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:30:21,867] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-03 11:30:21,867] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:21,868] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@7f5e973bdd66:44831
[2023-03-03 11:30:21,868] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-03 11:30:21,869] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-03 11:30:21,870] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-03 11:30:21,870] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-03 11:30:21,871] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,871] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:21,872] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:21,872] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,873] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,873] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,874] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,875] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:21,875] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:21,876] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:21,876] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:21,877] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,877] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:21,878] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:21,879] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,880] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,881] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,882] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,884] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:21,885] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:21,886] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:21,886] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:30:21,887] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,888] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-03 11:30:21,888] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-03 11:30:21,889] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:30:21,889] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-03 11:30:21,890] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-03 11:30:21,890] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-03 11:30:21,891] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-03 11:30:21,892] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-03 11:30:21,892] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-03 11:30:21,893] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,894] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,894] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,895] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,895] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-03 11:30:21,896] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-03 11:30:21,896] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-03 11:30:21,897] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-03 11:30:21,897] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-03 11:30:21,898] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-03 11:30:21,898] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:30:21,899] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:30:21,900] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:30:21,901] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:30:21,902] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:30:21,903] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:30:21,903] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:30:21,904] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:30:21,905] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:30:21,906] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:30:21,907] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-03 11:30:21,908] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-03 11:30:21,908] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-03 11:30:21,909] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-03 11:30:21,910] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-03 11:30:21,910] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-03 11:30:21,911] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:30:21,911] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:30:21,912] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:30:21,913] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:30:21,913] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:30:21,914] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:30:40,224] {spark_submit.py:495} INFO - 23/03/03 11:30:36 INFO Executor: Told to re-register on heartbeat
[2023-03-03 11:30:41,116] {spark_submit.py:495} INFO - 23/03/03 11:30:40 INFO BlockManager: BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None) re-registering with master
[2023-03-03 11:30:42,076] {spark_submit.py:495} INFO - 23/03/03 11:30:41 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None)
[2023-03-03 11:30:40,677] {base_job.py:229} ERROR - LocalTaskJob heartbeat got an exception
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 3141, in _wrap_pool_connect
    return fn()
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 301, in connect
    return _ConnectionFairy._checkout(self)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 755, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 419, in checkout
    rec = pool._do_get()
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/impl.py", line 259, in _do_get
    return self._create_connection()
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 247, in _create_connection
    return _ConnectionRecord(self)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 362, in __init__
    self.__connect(first_connect_check=True)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 605, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 72, in __exit__
    with_traceback=exc_tb,
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 599, in __connect
    connection = pool._invoke_creator(self)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 583, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/usr/local/lib/python3.7/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/airflow/jobs/base_job.py", line 201, in heartbeat
    session.merge(self)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2877, in merge
    _resolve_conflict_map=_resolve_conflict_map,
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2950, in _merge
    merged = self.get(mapper.class_, key[1], identity_token=key[2])
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2702, in get
    identity_token=identity_token,
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2800, in _get_impl
    load_options=load_options,
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/loading.py", line 535, in load_on_pk_identity
    bind_arguments=bind_arguments,
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1669, in execute
    conn = self._connection_for_bind(bind, close_with_result=True)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1520, in _connection_for_bind
    engine, execution_options
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 3095, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 91, in __init__
    else engine.raw_connection()
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 3174, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 3145, in _wrap_pool_connect
    e, dialect, self
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 2004, in _handle_dbapi_exception_noconnection
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 3141, in _wrap_pool_connect
    return fn()
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 301, in connect
    return _ConnectionFairy._checkout(self)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 755, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 419, in checkout
    rec = pool._do_get()
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/impl.py", line 259, in _do_get
    return self._create_connection()
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 247, in _create_connection
    return _ConnectionRecord(self)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 362, in __init__
    self.__connect(first_connect_check=True)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 605, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 72, in __exit__
    with_traceback=exc_tb,
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 599, in __connect
    connection = pool._invoke_creator(self)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 583, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/usr/local/lib/python3.7/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: http://sqlalche.me/e/14/e3q8)
[2023-03-03 11:32:37,008] {spark_submit.py:495} INFO - 23/03/03 11:32:20 ERROR Inbox: Ignoring error
[2023-03-03 11:32:37,009] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:32:37,010] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:32:37,011] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:32:37,011] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-03 11:32:37,012] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-03 11:32:37,012] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-03 11:32:37,013] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-03 11:32:37,014] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-03 11:32:37,014] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-03 11:32:37,015] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-03 11:32:37,015] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-03 11:32:37,016] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-03 11:32:37,017] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:32:37,017] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:32:37,018] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:32:37,018] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:32:37,019] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:32:37,020] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:32:37,021] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-03 11:32:37,021] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-03 11:32:37,022] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-03 11:32:37,023] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-03 11:32:37,024] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@7f5e973bdd66:44831
[2023-03-03 11:32:37,024] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-03 11:32:37,025] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-03 11:32:37,026] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-03 11:32:37,027] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-03 11:32:37,027] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:32:37,028] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:32:37,029] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:32:37,031] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:32:37,032] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2023-03-03 11:32:37,033] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2023-03-03 11:32:37,033] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2023-03-03 11:32:37,034] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2023-03-03 11:32:37,035] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2023-03-03 11:32:37,035] {spark_submit.py:495} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2023-03-03 11:32:37,036] {spark_submit.py:495} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2023-03-03 11:32:37,037] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2023-03-03 11:32:37,037] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2023-03-03 11:32:37,038] {spark_submit.py:495} INFO - ... 19 more
[2023-03-03 11:32:37,039] {spark_submit.py:495} INFO - 23/03/03 11:32:20 WARN Executor: Issue communicating with driver in heartbeater
[2023-03-03 11:32:37,040] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:32:37,041] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:32:37,042] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:32:37,042] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-03-03 11:32:37,050] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-03-03 11:32:37,051] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2023-03-03 11:32:37,051] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-03-03 11:32:37,052] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-03-03 11:32:37,053] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-03-03 11:32:37,054] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:32:37,055] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-03-03 11:32:37,056] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-03-03 11:32:37,056] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:32:37,057] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-03-03 11:32:37,058] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-03-03 11:32:37,059] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-03 11:32:37,060] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-03 11:32:37,061] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-03 11:32:37,062] {spark_submit.py:495} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:32:37,063] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:32:37,063] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:32:37,064] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-03 11:32:37,064] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-03 11:32:37,065] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-03 11:32:37,065] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-03 11:32:37,066] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-03 11:32:37,067] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-03 11:32:37,068] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-03 11:32:37,069] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-03 11:32:37,069] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-03 11:32:37,070] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:32:37,071] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:32:37,072] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:32:37,072] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:32:37,073] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:32:37,074] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:32:37,074] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-03 11:32:37,076] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:32:37,078] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@7f5e973bdd66:44831
[2023-03-03 11:32:37,079] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-03 11:32:37,080] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-03 11:32:37,084] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-03 11:32:37,084] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-03 11:32:37,085] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:32:37,086] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:32:37,086] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:32:37,087] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:32:37,088] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2023-03-03 11:32:37,088] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2023-03-03 11:32:37,089] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2023-03-03 11:32:37,089] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2023-03-03 11:32:37,090] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2023-03-03 11:32:37,090] {spark_submit.py:495} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2023-03-03 11:32:37,091] {spark_submit.py:495} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2023-03-03 11:32:37,092] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2023-03-03 11:32:37,092] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2023-03-03 11:32:37,093] {spark_submit.py:495} INFO - ... 19 more
[2023-03-03 11:32:37,094] {spark_submit.py:495} INFO - 23/03/03 11:32:37 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@9f81c1)) by listener AppStatusListener took 15.4784055s.
[2023-03-03 11:32:37,094] {spark_submit.py:495} INFO - 23/03/03 11:32:37 INFO Executor: Told to re-register on heartbeat
[2023-03-03 11:32:37,095] {spark_submit.py:495} INFO - 23/03/03 11:32:37 INFO BlockManager: BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None) re-registering with master
[2023-03-03 11:32:37,096] {spark_submit.py:495} INFO - 23/03/03 11:32:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None)
[2023-03-03 11:32:37,097] {spark_submit.py:495} INFO - 23/03/03 11:32:37 WARN Executor: Issue communicating with driver in heartbeater
[2023-03-03 11:32:37,099] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:32:37,100] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:32:37,100] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:32:37,101] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-03-03 11:32:37,102] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-03-03 11:32:37,102] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2023-03-03 11:32:37,103] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-03-03 11:32:37,103] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-03-03 11:32:37,104] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-03-03 11:32:37,104] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:32:37,105] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-03-03 11:32:37,105] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-03-03 11:32:37,106] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:32:37,107] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-03-03 11:32:37,107] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-03-03 11:32:37,108] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-03 11:32:37,109] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-03 11:32:37,110] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-03 11:32:37,111] {spark_submit.py:495} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:32:37,111] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:32:37,112] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:32:37,113] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-03 11:32:37,113] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-03 11:32:37,114] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-03 11:32:37,114] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-03 11:32:37,115] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-03 11:32:37,116] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-03 11:32:37,116] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-03 11:32:37,117] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-03 11:32:37,117] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-03 11:32:37,118] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:32:37,118] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:32:37,119] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:32:37,119] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:32:37,120] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:32:37,120] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:32:37,121] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-03 11:32:37,121] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:32:37,122] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@7f5e973bdd66:44831
[2023-03-03 11:32:37,122] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-03 11:32:37,123] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-03 11:32:37,123] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-03 11:32:37,124] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-03 11:32:37,124] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:32:37,125] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:32:37,125] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:32:37,126] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:32:37,126] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:32:37,127] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:32:37,127] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:32:37,128] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:32:37,128] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:32:37,129] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:32:37,129] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:32:37,130] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:32:37,131] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:32:37,131] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:32:37,132] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:32:37,133] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:32:37,133] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:32:37,134] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:32:37,135] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:32:37,135] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:32:37,136] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:32:37,137] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:32:37,139] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:32:37,139] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-03 11:32:37,141] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-03 11:32:37,142] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:32:37,143] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-03 11:32:37,144] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-03 11:32:37,144] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-03 11:32:37,145] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-03 11:32:37,145] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-03 11:32:37,146] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-03 11:32:37,147] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:32:37,148] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:32:37,148] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:32:37,149] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:32:37,150] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-03 11:32:37,151] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-03 11:32:37,151] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-03 11:32:37,152] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-03 11:32:37,152] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-03 11:32:37,153] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-03 11:32:37,154] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:32:37,154] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:32:37,155] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:32:37,155] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:32:37,156] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:32:37,156] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:32:37,157] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:32:37,158] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:32:37,158] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:32:37,159] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:32:37,159] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-03 11:32:37,160] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-03 11:32:37,161] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-03 11:32:37,162] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-03 11:32:37,163] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-03 11:32:36,862] {base_job.py:229} ERROR - LocalTaskJob heartbeat got an exception
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 3141, in _wrap_pool_connect
    return fn()
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 301, in connect
    return _ConnectionFairy._checkout(self)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 755, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 419, in checkout
    rec = pool._do_get()
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/impl.py", line 259, in _do_get
    return self._create_connection()
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 247, in _create_connection
    return _ConnectionRecord(self)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 362, in __init__
    self.__connect(first_connect_check=True)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 605, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 72, in __exit__
    with_traceback=exc_tb,
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 599, in __connect
    connection = pool._invoke_creator(self)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 583, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/usr/local/lib/python3.7/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/airflow/jobs/base_job.py", line 201, in heartbeat
    session.merge(self)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2877, in merge
    _resolve_conflict_map=_resolve_conflict_map,
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2950, in _merge
    merged = self.get(mapper.class_, key[1], identity_token=key[2])
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2702, in get
    identity_token=identity_token,
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2800, in _get_impl
    load_options=load_options,
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/loading.py", line 535, in load_on_pk_identity
    bind_arguments=bind_arguments,
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1669, in execute
    conn = self._connection_for_bind(bind, close_with_result=True)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1520, in _connection_for_bind
    engine, execution_options
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 3095, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 91, in __init__
    else engine.raw_connection()
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 3174, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 3145, in _wrap_pool_connect
    e, dialect, self
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 2004, in _handle_dbapi_exception_noconnection
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 3141, in _wrap_pool_connect
    return fn()
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 301, in connect
    return _ConnectionFairy._checkout(self)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 755, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 419, in checkout
    rec = pool._do_get()
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/impl.py", line 259, in _do_get
    return self._create_connection()
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 247, in _create_connection
    return _ConnectionRecord(self)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 362, in __init__
    self.__connect(first_connect_check=True)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 605, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 72, in __exit__
    with_traceback=exc_tb,
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 599, in __connect
    connection = pool._invoke_creator(self)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 583, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/usr/local/lib/python3.7/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: http://sqlalche.me/e/14/e3q8)
[2023-03-03 11:32:37,164] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-03 11:32:37,164] {local_task_job.py:144} ERROR - Heartbeat time limit exceeded!
[2023-03-03 11:32:37,165] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:32:37,166] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:32:37,167] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:32:37,167] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:32:37,168] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:32:37,169] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:32:37,169] {spark_submit.py:495} INFO - 23/03/03 11:32:37 ERROR Inbox: Ignoring error
[2023-03-03 11:32:37,170] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:32:37,170] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:32:37,171] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:32:37,172] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2023-03-03 11:32:37,173] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2023-03-03 11:32:37,174] {spark_submit.py:495} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2023-03-03 11:32:37,175] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2023-03-03 11:32:37,176] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2023-03-03 11:32:37,177] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)
[2023-03-03 11:32:37,177] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)
[2023-03-03 11:32:37,178] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)
[2023-03-03 11:32:37,178] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2023-03-03 11:32:37,179] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:32:37,180] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:32:37,180] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:32:37,181] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:32:37,182] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:32:37,182] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-03-03 11:32:37,183] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-03-03 11:32:37,184] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-03-03 11:32:37,184] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-03-03 11:32:37,185] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-03-03 11:32:37,186] {spark_submit.py:495} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@7f5e973bdd66:44831
[2023-03-03 11:32:37,186] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2023-03-03 11:32:37,187] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2023-03-03 11:32:37,188] {spark_submit.py:495} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2023-03-03 11:32:37,189] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2023-03-03 11:32:37,189] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:32:37,190] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:32:37,190] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:32:37,191] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:32:37,191] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:32:37,192] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:32:37,193] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:32:37,193] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:32:37,194] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:32:37,194] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:32:37,195] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:32:37,195] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:32:37,196] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:32:37,197] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:32:37,197] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:32:37,198] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:32:37,199] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:32:37,200] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:32:37,200] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:32:37,201] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:32:37,202] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:32:37,202] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2023-03-03 11:32:37,203] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:32:37,203] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2023-03-03 11:32:37,204] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2023-03-03 11:32:37,205] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-03-03 11:32:37,206] {spark_submit.py:495} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2023-03-03 11:32:37,206] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2023-03-03 11:32:37,207] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2023-03-03 11:32:37,208] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2023-03-03 11:32:37,208] {spark_submit.py:495} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2023-03-03 11:32:37,209] {spark_submit.py:495} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2023-03-03 11:32:37,210] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:32:37,210] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:32:37,211] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:32:37,211] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:32:37,212] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2023-03-03 11:32:37,213] {spark_submit.py:495} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2023-03-03 11:32:37,214] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2023-03-03 11:32:37,215] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2023-03-03 11:32:37,215] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2023-03-03 11:32:37,216] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2023-03-03 11:32:37,217] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2023-03-03 11:32:37,217] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2023-03-03 11:32:37,218] {spark_submit.py:495} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2023-03-03 11:32:37,219] {spark_submit.py:495} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2023-03-03 11:32:37,220] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2023-03-03 11:32:37,221] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2023-03-03 11:32:37,221] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2023-03-03 11:32:37,222] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2023-03-03 11:32:37,222] {spark_submit.py:495} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2023-03-03 11:32:37,223] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2023-03-03 11:32:37,224] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2023-03-03 11:32:37,225] {spark_submit.py:495} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2023-03-03 11:32:37,226] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2023-03-03 11:32:37,230] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2023-03-03 11:32:37,243] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2023-03-03 11:32:37,244] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2023-03-03 11:32:37,245] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-03-03 11:32:37,245] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-03-03 11:32:37,246] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-03-03 11:32:37,247] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-03-03 11:32:37,247] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-03-03 11:32:37,248] {spark_submit.py:495} INFO - ... 3 more
[2023-03-03 11:32:37,257] {spark_submit.py:495} INFO - 23/03/03 11:32:37 INFO Executor: Told to re-register on heartbeat
[2023-03-03 11:32:37,260] {spark_submit.py:495} INFO - 23/03/03 11:32:37 INFO BlockManager: BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None) re-registering with master
[2023-03-03 11:32:37,261] {spark_submit.py:495} INFO - 23/03/03 11:32:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7f5e973bdd66, 35627, None)
[2023-03-03 11:32:37,262] {spark_submit.py:495} INFO - 23/03/03 11:32:37 WARN Executor: Issue communicating with driver in heartbeater
[2023-03-03 11:32:37,262] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-03-03 11:32:37,262] {process_utils.py:129} INFO - Sending Signals.SIGTERM to group 905. PIDs of all processes in the group: [906, 961, 905]
[2023-03-03 11:32:37,263] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-03-03 11:32:37,264] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-03-03 11:32:37,266] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-03-03 11:32:37,266] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-03-03 11:32:37,267] {process_utils.py:80} INFO - Sending the signal Signals.SIGTERM to group 905
[2023-03-03 11:32:37,267] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2023-03-03 11:32:37,388] {taskinstance.py:1889} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/home/***/logs/dag_id=avg_product_price/run_id=manual__2023-03-03T11:27:04.771450+00:00/task_id=spark_submit/attempt=1.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/usr/local/lib/python3.7/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 414, in submit
    self._process_spark_submit_log(iter(self._submit_sp.stdout))  # type: ignore
  File "/usr/local/lib/python3.7/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 495, in _process_spark_submit_log
    self.log.info(line)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1378, in info
    self._log(INFO, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1029, in emit
    self.flush()
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1541, in signal_handler
    self.log.error("Received SIGTERM. Terminating subprocesses.")
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1407, in error
    self._log(ERROR, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 894, in handle
    self.emit(record)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 67, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1127, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1033, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 946, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 127, in write
    self.flush()
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 134, in flush
    self._propagate_log(buf)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/logging_mixin.py", line 115, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1444, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1513, in _log
    exc_info, func, extra, sinfo)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1483, in makeRecord
    sinfo)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 312, in __init__
    self.levelname = getLevelName(level)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 132, in getLevelName
    result = _levelToName.get(level)
RecursionError: maximum recursion depth exceeded while calling a Python object
[2023-03-03 11:32:37,618] {process_utils.py:75} INFO - Process psutil.Process(pid=961, status='terminated', started='11:27:15') (961) terminated with exit code None
[2023-03-03 11:32:37,765] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=avg_product_price, task_id=spark_submit, execution_date=20230303T112704, start_date=20230303T113021, end_date=20230303T113237
[2023-03-03 11:32:37,864] {standard_task_runner.py:97} ERROR - Failed to execute job 9 for task spark_submit (maximum recursion depth exceeded while calling a Python object; 905)
[2023-03-03 11:32:37,974] {process_utils.py:75} INFO - Process psutil.Process(pid=905, status='terminated', exitcode=1, started='11:27:07') (905) terminated with exit code 1
[2023-03-03 11:33:37,174] {process_utils.py:143} WARNING - process psutil.Process(pid=906, name='java', status='zombie', started='11:27:08') did not respond to SIGTERM. Trying SIGKILL
[2023-03-03 11:33:37,177] {process_utils.py:80} INFO - Sending the signal Signals.SIGKILL to group 905
[2023-03-03 11:34:37,211] {process_utils.py:154} ERROR - Process psutil.Process(pid=906, name='java', status='zombie', started='11:27:08') (906) could not be killed. Giving up.
